# Формирование нового датасета на основе Hugging Face

- [Формирование нового датасета на основе Hugging Face](#формирование-нового-датасета-на-основе-hugging-face)
  - [1. Введение](#1-введение)
    - [Цель](#цель)
    - [Источник данных](#источник-данных)
  - [2. Исходные датасеты](#2-исходные-датасеты)
    - [2.2. justinqbui/covid\_fact\_checked\_google\_api](#22-justinqbuicovid_fact_checked_google_api)
    - [2.3. justinqbui/covid\_fact\_checked\_polifact](#23-justinqbuicovid_fact_checked_polifact)
    - [2.4 Синтетика данных](#24-синтетика-данных)
      - [1. Генерация синтетических фактов](#1-генерация-синтетических-фактов)
    - [2.5 Описание комментариев](#25-описание-комментариев)
      - [2.5.1 **beenakurian/reddit\_comments\_subreddit\_canada**](#251-beenakurianreddit_comments_subreddit_canada)
      - [2.5.2 **AiresPucrs/toxic-comments**](#252-airespucrstoxic-comments)
      - [2.5.3 **Описание датасета `gxb912/large-twitter-tweets-sentiment`**](#253-описание-датасета-gxb912large-twitter-tweets-sentiment)
      - [3. Процесс генерации](#3-процесс-генерации)
      - [3. Преимущества добавления синтетики](#3-преимущества-добавления-синтетики)
      - [4. Дополнения](#4-дополнения)
  - [4. План объединения датасетов](#4-план-объединения-датасетов)
    - [4.1. Общие шаги](#41-общие-шаги)


## 1. Введение
### Цель
Цель — создать новый датасет, объединяющий несколько существующих датасетов, связанных с проверкой фактов о COVID-19. Это позволит использовать более полный и разнообразный набор данных для задач классификации, проверки фактов и анализа дезинформации.

### Источник данных
Датасеты взяты с платформы [Hugging Face](https://huggingface.co/), известной своей библиотекой для работы с данными и моделями машинного обучения.

## 2. Исходные датасеты

Датасет `nanyy1025/covid_fake_news` на Hugging Face содержит 10 700 записей, каждая из которых представляет собой твит, связанный с COVID-19, с меткой "real" (реальный) или "fake" (фейковый). Датасет разделен на три части: обучающая выборка (6 420 записей), валидационная выборка (2 140 записей) и тестовая выборка (2 140 записей). Данные представлены в формате CSV и предназначены для задач классификации текста и zero-shot классификации. Датасет был использован в исследовании "Fighting an Infodemic: COVID-19 Fake News Dataset" (arXiv:2011.03327). ([Hugging Face](https://huggingface.co/datasets/nanyy1025/covid_fake_news?utm_source=chatgpt.com)) 

```bibtex
@misc{patwa2020fighting,
title={Fighting an Infodemic: COVID-19 Fake News Dataset}, 
author={Parth Patwa and Shivam Sharma and Srinivas PYKL and Vineeth Guptha and Gitanjali Kumari and Md Shad Akhtar and Asif Ekbal and Amitava Das and Tanmoy Chakraborty},
year={2020},
eprint={2011.03327},
archivePrefix={arXiv},
primaryClass={cs.CL}
}
```

### 2.2. justinqbui/covid_fact_checked_google_api

Датасет **justinqbui/covid_fact_checked_google_api** с Hugging Face представляет собой выборку проверенных фактов, связанных с COVID-19, собранных с использованием Google Fact Checker API. Вот его основные характеристики:

1. **Общий объем данных:**
   - Содержит 3 043 записи.
   - Первоначально было собрано 10 000 фактов, но для упрощения включены только те записи, где рейтинг был представлен одним словом — "false" (ложь) или "true" (правда). Около 90% фактов в датасете оценены как ложные.

2. **Модальности и форматы:**
   - Тип данных: текст.
   - Формат: CSV.
   - Также доступен в формате Parquet.

3. **Описание данных:**
   - Поля датасета:
     - `text`: текст проверенного факта.
     - `label`: метка правдивости (`true` или `false`).
   - Аннотации созданы экспертами.
   - Язык: английский (en-US).
   - Датасет является монолингвальным.

### 2.3. justinqbui/covid_fact_checked_polifact

Датасет **justinqbui/covid_fact_checked_polifact** с Hugging Face включает записи, связанные с проверкой фактов о COVID-19, собранных с помощью автоматического веб-скрейпера, который извлек данные из PolitiFact COVID Fact Checker. Вот его ключевые характеристики:

1. **Общий объем данных:**
   - Содержит 1 190 записей.
   - Датасет состоит из утверждений и их оценки на правдивость.

2. **Модальности и форматы:**
   - Тип данных: текст.
   - Формат: CSV.
   - Также доступен в формате Parquet.

3. **Описание данных:**
   - Поля датасета:
     - `claim`: текст утверждения.
     - `rating`: оценка, присвоенная PolitiFact (7 значений: *half-true*, *full-flop*, *pants-fire*, *barely-true*, *true*, *mostly-true*, *false*).
     - `adjusted_rating`: упрощенная версия оценки (3 значения: например, "true", "false" и промежуточные варианты).
   - Поля предоставляют как детализированные, так и агрегированные оценки фактов.

### 2.4 Синтетика данных

В дополнение к объединению данных из существующих датасетов, мы планируем сгенерировать синтетические данные, которые будут включены в результирующий датасет. Это позволит увеличить разнообразие записей, сделать датасет более сбалансированным и улучшить способность моделей обобщать информацию на новых данных.

#### 1. Генерация синтетических фактов
- **Источник генерации:** Для создания синтетических данных будет использована модель GPT (например, GPT-4), обладающая способностью генерировать текст на основе заданных параметров.
- **Типы фактов:**
  - **Ложные факты:** Сформулируем неправдивые утверждения о коронавирусе и вакцинации, отражающие распространенные заблуждения и мифы, например:
    - Вакцины содержат чипы для отслеживания людей.
    - Коронавирус был создан в лаборатории как биологическое оружие.
  - **Достоверные факты:** Добавим научно обоснованные утверждения, такие как:
    - Вакцины против COVID-19 были проверены в ходе крупных клинических испытаний.
    - Мытье рук с мылом помогает снизить риск передачи вируса.
- **Объем:** Планируется создать около 1 000 записей (примерно 500 ложных и 500 достоверных фактов) для увеличения объема и разнообразия данных.

### 2.5 Описание комментариев

#### 2.5.1 **beenakurian/reddit_comments_subreddit_canada**
Этот датасет содержит комментарии из канадского сабреддита на Reddit. 

- **Объем данных:** 6 908 записей.
- **Формат:** CSV.
- **Размер:** 
  - Исходный файл: 453 кБ.
  - Parquet-версия: 295 кБ.
- **Язык:** Английский.
- **Структура данных:**
  - `comment`: текст комментария.
  - `sentiment`: метка настроения (предположительно позитивное, нейтральное или негативное).
- **Задачи:** Классификация текста.
- **Лицензия:** Неизвестна.

#### 2.5.2 **AiresPucrs/toxic-comments**
Этот датасет предназначен для анализа токсичных комментариев и используется в образовательных целях для исследования AI Ethics и Safety.

- **Объем данных:** 70 157 записей.
- **Формат:** CSV.
- **Размер:** 
  - Исходный файл: 15.5 МБ.
  - Parquet-версия: 8.68 МБ.
- **Язык:** Английский.
- **Структура данных:**
  - `comment_text`: текст комментария.
  - Метки токсичности (подробная информация не указана, но вероятно, что классификация включает токсичные и нетоксичные комментарии).
- **Задачи:** Классификация токсичности текста.
- **Лицензия:** Другая (не уточнена).
  
1. **Добавление в объединенный датасет:** 
   - Ввести колонку `source`, чтобы указывать происхождение данных (`reddit_canada`, `toxic_comments`).
   - Сохранить оригинальные метки (`sentiment`, `is_toxic`) в `original_label_1` для последующего анализа.

#### 2.5.3 **Описание датасета `gxb912/large-twitter-tweets-sentiment`**

**Общая информация**
- **Название:** Large Twitter Tweets Sentiment Analysis
- **Источник:** [Hugging Face](https://huggingface.co/datasets/gxb912/large-twitter-tweets-sentiment)
- **Задача:** Классификация текста (Sentiment Analysis)
- **Модальности:** Текст
- **Формат:** CSV
- **Язык:** Английский (English)
- **Размер:** 100K - 1M записей
- **Лицензия:** MIT

**Описание данных**
Данный датасет представляет собой коллекцию твитов, размеченных для задачи анализа сентимента. Каждый твит сопровождается меткой настроения:

- `1` — положительный (`Positive`).
- `0` — отрицательный (`Negative`).

**Структура данных**
Каждая запись содержит два поля:
- **`text`**: строка с содержимым твита.
- **`sentiment`**: числовое значение, указывающее на настроение твита (`0` — негативное, `1` — позитивное).

**Разбиение и объем данных**
Датасет поделен на две части:
- **Обучающая выборка:** 179 995 записей.
- **Тестовая выборка:** 44 999 записей.

---

#### 3. Процесс генерации
1. **Формирование шаблонов:** Определим набор шаблонов для генерации утверждений:
   - Примеры для ложных фактов: "Некоторые утверждают, что ___, но это не подтверждено наукой."
   - Примеры для достоверных фактов: "Исследования показали, что ___."
2. **Контроль качества:**
   - Все сгенерированные данные будут проверены вручную для исключения бессмысленных или неоднозначных утверждений.
   - Будет уделено внимание тому, чтобы синтетические данные не дублировали реальные записи из оригинальных датасетов.
3. **Метаданные:** Каждой записи будет добавлена информация о том, что она синтетическая (например, в столбец `source` будет добавлено значение `synthetic`).

#### 3. Преимущества добавления синтетики
- **Разнообразие данных:** Добавление новых записей увеличивает покрытие различных типов утверждений, включая менее распространенные и нетипичные примеры.
- **Сбалансированность:** Можно целенаправленно генерировать факты для недопредставленных классов, что поможет устранить дисбаланс данных.
- **Обучение моделей:** Синтетика помогает улучшить способность моделей различать правду и ложь, особенно в случае редких или новых типов утверждений.

#### 4. Дополнения
- **Тематическая направленность:** Генерация синтетических данных будет учитывать различные аспекты COVID-19, такие как:
  - Симптомы и профилактика.
  - Мифы о вакцинации.
  - Политические и социальные аспекты пандемии.
- **Добавление сложности:** Помимо простых утверждений, будут добавлены более сложные многофакторные утверждения (например, включающие статистику, ссылки на "исследования").
- **Лингвистическая вариативность:** Генерация фактов будет учитывать вариативность формулировок для снижения зависимости модели от конкретных шаблонов.

**Нейтральные фейковые утверждения**
- Напиши 50 нейтральных примеров фейковых утверждений о коронавирусе. Факты должны состоять из 2 предложений.  
- Напиши 50 коротких фейковых утверждений о вакцинации против коронавируса. Каждое утверждение должно быть длиной 1 предложение.  
- Напиши 50 фейковых утверждений о распространении коронавируса. Каждый текст должен состоять из 3 предложений.  

**Правдивые утверждения**
- Напиши 50 правдивых утверждений о влиянии вакцинации на снижение уровня заражений коронавирусом. Каждое утверждение должно быть длиной 1 предложение.  
- Напиши 50 правдивых утверждений о способах профилактики коронавируса. Каждый текст должен содержать 3 предложения.  
- Напиши 50 правдивых фактов о текущих исследованиях, связанных с коронавирусом. Каждое утверждение должно состоять из 2 предложений.  

**Сравнительные утверждения**
- Напиши 50 сравнительных утверждений, где фейковые факты противопоставляются правдивым. Каждый текст должен состоять из 3 предложений.  
- Напиши 50 утверждений, в которых одно предложение будет правдивым фактом о вакцинации, а второе — фейковым.  

**Разные цели и тональность**
- Напиши 50 примеров сенсационных фейковых заявлений о вакцинации против коронавируса. Каждый текст должен быть из 1 предложения.  
- Напиши 50 научных утверждений о коронавирусе, которые звучат правдоподобно, но являются ложными. Тексты должны состоять из 2 предложений.  
- Напиши 50 правдивых утверждений о коронавирусе, которые предназначены для образовательных целей. Каждый текст должен быть длиной 3 предложения.  

## 4. План объединения датасетов
### 4.1. Общие шаги
1. **Загрузка данных:** Использовать библиотеку `datasets` для загрузки всех трех датасетов.
2. **Предварительная обработка:**
   - Привести все датасеты к единому формату (например, текст + метка).
   - Удалить дубликаты записей.
   - Убедиться, что значения меток согласуются между датасетами.
3. **Объединение:**
   - Объединить записи из всех датасетов в одну таблицу.
   - Добавить столбец для указания источника каждой записи (например, `source`).
4. **Балансировка данных:**
   - Проверить распределение классов и при необходимости сбалансировать данные.
5. **Сохранение:**
   - Сохранить объединенный датасет в формате CSV или Parquet.