# Транскрипция

(0:00) опишем весь пайплайн нашей работы. У нас есть датасет загруженный с (0:09) фейсбука о комментариях и диалогах про ковид, вакцинацию, нетработников. (0:19) Первым делом мы обрабатываем датасет для того чтобы можно было с ним корректно (0:23) работать.
Мы загружаем датасет, для этого мы используем пандас, преимущественно (0:29) пандас. Загружаем датасет, смотрим какие в нем есть колонки. В нашем датасете (0:37) колонки и его размер.
В нашем датасете 6868 строк. Анализируя датасет по строкам мы (0:47) можем увидеть закономерность, что выгруженные сообщения имеют особенность, что (1:03) некоторые из них загружены в фейсбуке как текст поста, это с колонкой message и (1:14) пользователи иногда цитируют источники из других ресурсов как New York Times или другие (1:20) новостные порталы. Из этого мы видим, что есть колонка message, где текст именно поста из (1:28) фейсбука и колонка text link и link message, нет description.
Тем самым мы можем сформировать (1:39) две дополнительные колонки, с которыми мы будем работать в дальнейшем. Мы создаем новую (1:47) колонку text, которая равна колонке message, это конкретный текст поста, и колонка link text, (1:57) которая состоит из конкретной колонки link text и description. Тем самым мы формируем две (2:05) колонки, с которыми будем работать, при этом в некоторых колонках они пропускаются, это может (2:11) быть как и в строках может быть и текст и link text, а в некоторых могут быть пропуски, с которыми (2:18) мы потом дальше еще их обработаем.
Формируем датасет и выгружаем его, то есть у нас будет (2:25) две последующие работы. Следующим этапом мы хотим проанализировать наш датасет, сформировать (2:32) комбининги и пластеризовать, то есть мы еще не используем какую-то предобученную модель, мы просто (2:38) формируем комбининги на основе существующей модели, для этого мы используем Roberto. Roberto (2:45) мы будем загружать с платформы Hugging Face, Roberto вообще доступная и обычно в достаточно (2:54) большом корпусе и мы посмотрим, есть ли какие-то закономерности при таком использовании модели.
(3:08) Значит важным этапом будет, что мы используем, нужно знать точно какой максимальная длина (3:14) последовательности, которую мы будем обрабатывать, то есть 128 токенов длина, то есть если текст (3:22) делится, при кодировании используется токенизатор, то есть слова не как слова идут, а индексы (3:32) токенов, то есть слова бьются на токены, какие-то несколько допустим последовательных букв, которые (3:39) формируют один токен и тем самым мы с предложения текст кодируем в некоторую последовательность индексов, (3:48) который как раз является токенами или последовательность токенов. Мы загружаем датасеп, который мы (3:56) только что подготовили с фейсбука, подготавливаем как-то данные, то есть мы указываем, что (4:07) текстовая колонка текст имеет тип string, обрабатываем строки, в которых у нас нет текста, (4:17) то есть такой тоже может быть тонан или пустое значение какое-то и добавляем еще ID для (4:24) вывода для аналитики на графике. Итого, загружаем модель с HuggingFace, обычную Roberta, недообученную, (4:39) далее мы формируем имбединги, то есть мы пишем отдельный класс имбедингов, при этом мы можем (4:45) разные имбединги использовать, CLS, MIN, MAX, ну или SUM, это берется последний слой модели, (4:55) не используя классификатор, который идет поверх обычно модели, мы работаем только с основными (5:03) слоями, точнее даже с последним LastHiddenState и используем его, то есть мы на выходе получаем (5:10) имбединг равный вектор 768, получаем имбендинг, далее мы его сохраняем локально на машине, (5:25) это делается для того, чтобы, допустим, если это делать на ноутбуке, мы могли закомментировать, (5:31) допустим, часть кода и уже не заново проводить токенизацию всех слов и прогонять их через (5:40) модель, а использовать уже сохраненную в Facebook Embeddings, дальше мы выполняем уменьшение (5:48) размерности тремя способами вариации, делаем отдельный класс и прописываем все методы, то есть у нас (5:56) есть три метода, это метод главного компонента PCI, метод T-SNE и метод UMAP, по них подробнее можно (6:05) будет почитать, посмотреть, что они себе представляют, но нас интересует сам результат, также мы будем (6:12) визуализировать с помощью библиотеки Plotly, потому что она позволяет интерактивно смотреть данные, можно, (6:20) конечно, делать на Plotly, но мы используем Plotly просто для интерактива, чтобы было проще анализировать (6:25) результаты, посмотреть конкретные кластеры, приблизить, почитать какие сообщения, что писали людям, (6:34) и при визуализации мы видим метод PCI, особой какой-то структуры данных нету, то есть данные (6:44) распределены, можно видеть, выделить два каких-то больших кластера, но как-то конкретней сложно (6:52) оценить корректность этих данных, то есть мы используем Rovert, она обучалась на большом корпусе, (7:01) то есть там нет определенной... модель обучалась не для какой-то компьютерной сдачи, а просто для (7:13) для большого корпуса, возможно, для многих задач.
Идем дальше, метод T-SNE тут уже чуть получше, (7:20) видно какие-то более выделенные кластеры, еще мы перед этим сделали некоторую прозрачность, то есть меток (7:29) на графике, если они стоят близко друг к другу, они более ярко выражены, тем самым мы можем выделить (7:37) некоторые кластеры. Более прозрачные метки, значит, они не стоят близко друг к другу. И метод UMAP (7:44) тоже можно видеть, он не такой разряженный, видно два больших кластера и несколько более мелких.
(7:52) Их можно приблизить, посмотреть, почитать, что значит, изучить текст. Следующая у нас (8:03) гипотеза возникает, а что если модель дообучить и дообучить для конкретной нашей задачи, то есть на (8:11) основе текста про COVID фейковые данные про COVID, недостоверные и достоверные факты. При этом, когда мы читали, (8:21) просматривали дату Facebook, DataFrame Facebook, мы заметили, что очень много комментариев, не имеющих (8:29) какой-то конкретной значимости, то есть это какие-то комментарии жизненной ситуации, просто общение людей, (8:35) либо цитирование каких-то фактов, не относящихся к ковиду, вакцинации, медицине.
Для этого мы предполагаем (8:49) сформировать новый датасет на уже существующих каких-то данных, может быть, симулировать данные и обучить (8:55) модель на классификации по трем классам. Это фейк, действительный факт и комментарий. Для этого мы формируем (9:05) датасет.
Все данные мы берем с Hugging Face. Первая модель — это у нас фейковые новости. Это модель, эти данные (9:18) мы используем для обучения модели, для классификации новостных каких-то паблик, новостных статей на предмет действительно новости (9:35) или фейковая.
Также мы использовали второй датасет — это факты о ковиде. В первом датасете у нас порядком 16 тысяч (9:49) или 17 тысяч сообщений. Во втором датасете это порядком 3 тысяч сообщений.
И третий датасет, который у нас про ковид, — это 1200 сообщений. (10:01) Тоже факты, там ложные и правдивые факты. Дальше в сумме у нас получается 17 тысяч сообщений.
Этого нам недостаточно. (10:14) Мы все данные объединяем. Также мы добавляем еще один датасет.
Мы хотим более целенаправленно добавить данные про ковид, вакцинацию. (10:33) Для этого мы синтезировали данные в генераторе ChatGPT. Мы формировали фейковые и нефейковые факты, подтвержденные ВОЗом, допустим, реальные факты о ковиде и вакцинации.
(10:50) Мы их записали в текстовом формате, просто в текстовый файл. То есть различные факты о вакцинации, о болезнях, негативные и положительные. (11:03) То есть у нас позитив — это правдивые факты про вакцинацию.
Допустим, что она уменьшает риск заболевания. Может, не уменьшает риск заболевания, а скорее уменьшает последствия после заболевания. (11:22) Что она не влияет на какие-то репродуктивные способности человека и так далее.
И негативные наоборот. Мы генерируем разные как и абсурдные факты, так и более реалистичные. Допустим, что мужчины болеют чаще ковидом и так далее.
(11:51) Мы формируем из этих данных датасет и еще формируем третью группу — это комментарии. В комментарии мы берем из-за отсутствия возможности разметить данные. (12:05) Для нормального обучения нужно большое количество данных, поэтому мы используем общедоступные датасеты с платформой Hugging Face.
Данные просто комментариев. (12:23) То есть, допустим, комментарии с Reddit мы используем. Токсичные комментарии и датасеты, которые используются для сантимента анализа.
(12:34) Конкретно нас не интересует суть этого комментария. Нас интересует просто создать некоторую массу сообщений, которые будут отвечать за комментарии, за разговоры людей ни о чем. (12:53) То есть это просто разговоры.
Тем самым мы пытаемся как бы выделить из массы сообщений те сообщения, которые нас менее интересуют. (13:05) Итого, мы объединяемся в эти три большие группы датасетов. То есть это данные про конкретно COVID — fake, не fake.
Данные, синтетические данные, которые мы сформировали в GPT и данные из данных комментариев. (13:23) Тем самым у нас получился датасет размером 300 тысяч комментариев, 9 тысяч фейковых сообщений и 600 реальных сообщений. (13:37) Также мы сформируем два датасета.
Один полностью неизмененный и датасет с удалением всех хештегов, упоминаний и ссылок для более корректного обучения. (13:56) Мы это все загружаем и сохраняем эти датасеты. Следующим этапом будет обучение нашей модели Роберто до обучения файтюнинг.
Что мы делаем? (14:11) Мы также загружаем модель. Важным будет установить максимальную длину последовательности сети. Для ноутбука я указывал 64, потому что чем больше последовательность токенов, которые мы можем загружать при обучении, тем больше текста мы можем загрузить в модель и проанализировать больше токенов.
(14:34) Однако это увеличивает в разы скорость обучения. Чем больше токенов, тем медленнее модель обучается. Для ноутбука я выбрал 64.
В целом это более корректно. (14:51) Батч сайз — это размер пакета, с которым будет обучаться. В модель будет подаваться не одно сообщение, а сразу пакетом несколько.
Я выбрал 16. Это более-менее корректно. Однако для удачного обучения лучше выбирать и большой размер длины последовательности, и большой пакет.
64 или 128. (15:11) Но это уже трудоемкая задача. Модель будет читаться час-полтора на ноутбук.
Нас это не устраивает. Поэтому я выбрал длину последовательности 64. Это можно в дальнейшем будет протестировать, перезапустить все модели, найти какое-то оптимальное решение, чтобы соблюдалась и точность, и время обучения было более-менее корректным.
(15:42) Мы загружаем датасет. Мы балансируем индексы. У нас в исходном датасете 300 тысяч сообщений и комментариев.
Это слишком много для обучения на ноутбуке без использования видеокарты. (16:03) Поэтому этот параметр можно будет выбрать самостоятельно. Я выбрал 30 тысяч.
То есть мы оставили в исходном датасете только 30 тысяч сообщений. Итого в нашем датасете получается 30 тысяч комментариев, 6,5 тысяч реальных фактов и 9 тысяч фейковых. (16:32) Мы предполагаем, что этого будет достаточно.
Однако это можно будет протестировать в дальнейшем и выбрать какое-то тоже оптимальное значение. Либо использовать весь датасет, но уже используем обучение на видеокарте. (16:46) Дальше мы делим датасет на тренировочный, тестовый и валидационный.
Тренировочный датасет нужен для обучения модели. Валидационный нужен для тестирования в процессе обучения. И тестовый датасет нам нужен только для тестирования модели уже после обучения.
(17:08) Далее мы токенизируем наши последовательности текстов, формируем датасет, уже токенизированный. Создаем даталодер. Даталодер нужен для модели, чтобы подавать пакетами как раз все значения.
(17:29) И мы собираем еще тренера. Тренер нужен для обучения модели. То есть это алгоритм обучения.
Там прописаны все параметры. Это модель, валидационный, тестовый датасет. Это данные оптимайзер.
(17:49) Это функция оптимизации критерий, по которому мы будем оценивать как смещать веса модели. И метрики. Метрику мы выбираем по шагам.
Количество шагов по метрикам 10. И девайс на котором будем обучать. В нашем случае это CPU, потому что у нас нет видеокарты.
(18:11) Я на ноутбуке это делаю, записываю. У меня нет видеокарты, я использую CPU, то есть на центральном процессоре. Здесь описаны алгоритмы, где что делается, комментарии подписаны.
И также есть функция под вызов результата метрики в процессе обучения. (18:33) Дальше мы обучаем. Скорость обучения 1 на 10 минус 5 степени.
Количество эпох 2. Я бы выставил 5, но в целом я думаю достаточно и двух эпох, потому что модель достаточно точно все классифицирует. Но можно это в дальнейшем будет и увеличить. (18:57) В нашем случае в качестве оптимизатора мы использовали Adam-V.
Критерий оценки Cross Entropy Loss. Можно конечно использовать и другой оптимайзер, Adagrad, Remsprobe, но это не принципиально. (19:18) В таких задачах пишут, что наиболее эффективно справляется Adam-V, поэтому мы не будем использовать другой оптимайзер.
Если есть необходимость, то можно заменить, но это не несет важности для дальнейших результатов. (19:56) Дальше мы обучаем модель и формируем графики. (20:03) На графиках у нас будет потеря при обучении, потеря при тестировании, при валидации.
И также будут метрики F1 Score и Accuracy. F1 Score это метрика очень похожая на точность. (20:21) Точность это полное совпадение меток при обучении и при предикшене модели.
А F1 Score это еще оценивается по категориям. (20:37) Насколько с категориями связано, она чуть поменьше, но при многоклассовой классификации более корректно оценивать модель, нежели через Accuracy. (20:49) Дальше мы сохраняем модель нашу для дальнейшего ее использования и тестируем датасет.
Точнее пишем класс для тестирования модели, мы загружаем тестовый датасет и проверяем то, как справляется модель на тестовом датасете. (21:10) Также мы написали небольшой класс для тестирования конкретно одного сообщения. То есть можно написать одно сообщение, прогнать через модель и получить результат, фейк, не фейк или комментарий.
(21:26) Даша точность при таком обучении при длине и последовательности 128 и размере пакета Batch Size 64 составляет порядка 90%, даже больше, на тестовом датасете. (21:52) Внизу также будет ознакомиться с датасетом, мы его также сохраняем, который тестовый. Для того, чтобы посмотреть результат датафрейм, можно посмотреть какие у нас конкретно почитать сообщения, посмотреть какой у нас вердикт, фейк, не фейк, коммент и посмотреть почему так произошло и выявить какие-то паттерны для дальнейшего улучшения.
(22:21) Можно сделать более качественную разметку для каких-то сообщений или использовать автоматизированные средства для разметки. Можно найти паттерны по ключевым словам этих сообщений, отфильтровать их и в дальнейшем их доработать, чтобы модель не совершала этих ошибок в дальнейшем. (22:48) Мы сохранили модель.
На тестовых данных, которые мы обучали модель, модель себя хорошо показывает. Теперь мы ее хотим проверить на данных из фейсбука. (22:59) Мы также загружаем модель, выбираем, устанавливаем обязательно длину последовательности, ту, которую мы использовали при обучении.
Размер пакета можно выставить больше в целом. Я имею в виду, что это независимые параметры. Главное тут длину последовательности.
(23:23) Устанавливаем ту, которую мы использовали при обучении. Дальше мы также эти данные токенизируем, данные из фейсбука, датасет фейсбука, мы его токенизируем, загружаем модель, токенизер, прогоняем через модель наши данные, получаем также DataFrame, TestResult DataFrame, которым мы также делаем выгрузку. (23:50) Сохраняем его для дальнейшего анализа или доработки.
Мы можем также зайти как в сам файл Excel, так и посмотреть визуально, вывести на экран. (24:22) Можно в своей машине все запустить, чтобы ты сама это видела, понимала, что как работает. Все проверено, оно все работает.
Можно будет посмотреть все данные также, где фейк, где не фейк, где комментарий, также проанализировать, насколько наша модель адекватно классифицирует посты, текстовые сообщения. (24:52) Следующим этапом у нас будет проверка уже данных кластеризации на основе той модели, которую мы обучили. Мы также используем датасет, который мы только что сохранили, фейсбук датасет, который подготовлен уже, уже размеченный моделью.
(25:14) Также загружаем модель нашу, Роберту, которую мы дообучили. Сформируем импеднинги, то есть у нас последний слой этой модели без классификатора. (25:29) И с помощью трех методов, также PCA, TSNE и UMAP, мы уменьшаем размерность и визуализируем сообщение.
Также мы при этом используем метки, которые мы разметили текст. (25:45) То есть, допустим, мы можем видеть уже на PCA, уже видны четкие границы, мы разделили весь датасет на три большие группы, большую часть занимает комментарии, фейки и действительные факты. (26:02) Для этого метода TSNE уже видно какие-то кластеры, видно, что сообщения как-то сгруппировались по небольшим группам, то есть можно уже, допустим, выделить какую-то группу и уже посмотреть конкретно, действительно ли сообщение относится к этой категории, о чем, имеет ли тут текста родственные какие-то связи, (26:28) какой-то может контекст сообщений совпадать между собой и также методы UMAP.
Также UMAP можно видеть тоже в целом неплохую группировку по группам, именно кластера можно их выделить и также посмотреть, за что отвечает конкретная группа, почитать, посмотреть, действительно ли это так. (27:00) На этом в целом все, то есть мы подготовили датасет, обучили модель и получили вот эти кластеры для визуализации. Следующим этапом это будет разметка данных для более четкого разделения, более четкого понимания, что нас интересует, чтобы модель четко могла различать те данные, которые нам нужны.