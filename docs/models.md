### Подход
Мы разработали модель для классификации текста на две категории: **реальные утверждения** и **фейковые утверждения** в контексте COVID-19. Основой проекта является использование предварительно обученной модели `RobertaForSequenceClassification` из библиотеки Hugging Face Transformers. 

Для обучения и тестирования модели текстовые данные предварительно обрабатывались с использованием токенизатора Roberta. Также применялись стандартные техники оптимизации гиперпараметров, чтобы добиться высокой точности и F1-метрики.

### Используемая модель
Мы выбрали предварительно обученную модель **RoBERTa** (Robustly Optimized BERT Pretraining Approach), которая хорошо себя зарекомендовала в задачах обработки естественного языка. Конкретная модель — `RobertaForSequenceClassification` с двумя классами (реальные и фейковые утверждения).

### RoBERTa: Подробное описание

**RoBERTa (Robustly Optimized BERT Pretraining Approach)** — это усовершенствование оригинальной модели BERT, предложенной в работе команды Facebook AI в 2019 году. RoBERTa использует ту же архитектуру, что и BERT, но включает несколько ключевых изменений, которые улучшают качество представлений и производительность модели на задачах NLP.

---

#### Основные улучшения RoBERTa:
1. **Объем данных**:  
   RoBERTa обучалась на значительно большем объеме данных, чем оригинальная BERT:
   - 160 ГБ текстов против 16 ГБ у BERT.
   - Источники данных включают BookCorpus, OpenWebText, CC-News и Stories.

2. **Длительность и масштаб предобучения**:  
   - Используются более длинные последовательности токенов и большее количество эпох для предобучения.
   - Модель тренируется с большей вычислительной мощностью и большими батчами.

3. **Маскирование**:  
   Вместо использования статического маскирования (как в BERT) применяется **динамическое маскирование**, где маски генерируются заново для каждой эпохи обучения. Это позволяет модели видеть больше разнообразных контекстов для каждого токена.

4. **Убрана задача NSP (Next Sentence Prediction)**:  
   В отличие от BERT, задача предсказания следующего предложения в RoBERTa исключена, так как она показала себя неэффективной. Вместо этого внимание сосредоточено исключительно на задаче маскированного языкового моделирования (Masked Language Modeling, MLM).

5. **Оптимизация гиперпараметров**:  
   RoBERTa использует тщательно подобранные гиперпараметры, такие как более высокие коэффициенты обучения и больший объем батчей, что улучшает качество предобучения.

---

#### Результаты
Благодаря этим изменениям RoBERTa достигает выдающихся результатов на многих NLP-бенчмарках, включая **GLUE**, **SuperGLUE**, **SQuAD**, **RACE** и других, превосходя оригинальный BERT и другие современные модели.

---

Оригинальная статья:  
**RoBERTa: A Robustly Optimized BERT Pretraining Approach**  
[https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692)

### Гиперпараметры
- **Максимальная длина токенов**: 128
- **Оптимизатор**: AdamW
- **Функция потерь**: `CrossEntropyLoss`
- **Размер батча**: 16
- **Коэффициент обучения**: \(5 \times 10^{-5}\)
- **Эпохи обучения**: 3
- **Warmup Steps**: 500
- **Весовая декомпозиция (Weight Decay)**: 0.01

Модель обучалась с использованием GPU для ускорения вычислений, при этом валидация выполнялась на тестовой выборке, где были достигнуты следующие результаты:
- **Точность (Accuracy)**: 93.43%
- **F1-мера**: 0.9343

### Оптимизатор AdamW
Оптимизатор **AdamW** (Adam с весовой декомпозицией) используется для улучшения стабильности обучения и предотвращения переобучения. Основное отличие от стандартного Adam — это добавление регуляризации (L2-регуляризация) через весовую декомпозицию, что делает обучение более устойчивым для глубоких моделей. Подробнее можно узнать из статьи: [Decoupled Weight Decay Regularization, ICLR 2019](https://arxiv.org/abs/1711.05101).

### Функция потерь
Для оптимизации обучения используется стандартная функция потерь `CrossEntropyLoss`, которая хорошо подходит для задач классификации, где классы представлены как one-hot метки. Эта функция вычисляет расстояние между предсказанными вероятностями и истинными метками. Подробности можно найти в документации PyTorch: [CrossEntropyLoss Documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).

  

