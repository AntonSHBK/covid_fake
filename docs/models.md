### Подход
Мы разработали модель для классификации текста на две категории: **реальные утверждения** и **фейковые утверждения** в контексте COVID-19. Основой проекта является использование предварительно обученной модели `RobertaForSequenceClassification` из библиотеки Hugging Face Transformers. 

Для обучения и тестирования модели текстовые данные предварительно обрабатывались с использованием токенизатора Roberta. Также применялись стандартные техники оптимизации гиперпараметров, чтобы добиться высокой точности и F1-метрики.

### Используемая модель
Мы выбрали предварительно обученную модель **RoBERTa** (Robustly Optimized BERT Pretraining Approach), которая хорошо себя зарекомендовала в задачах обработки естественного языка. Конкретная модель — `RobertaForSequenceClassification` с двумя классами (реальные и фейковые утверждения).

### RoBERTa: Подробное описание

**RoBERTa (Robustly Optimized BERT Pretraining Approach)** — это усовершенствование оригинальной модели BERT, предложенной в работе команды Facebook AI в 2019 году. RoBERTa использует ту же архитектуру, что и BERT, но включает несколько ключевых изменений, которые улучшают качество представлений и производительность модели на задачах NLP.

---

#### Основные улучшения RoBERTa:
1. **Объем данных**:  
   RoBERTa обучалась на значительно большем объеме данных, чем оригинальная BERT:
   - 160 ГБ текстов против 16 ГБ у BERT.
   - Источники данных включают BookCorpus, OpenWebText, CC-News и Stories.

2. **Длительность и масштаб предобучения**:  
   - Используются более длинные последовательности токенов и большее количество эпох для предобучения.
   - Модель тренируется с большей вычислительной мощностью и большими батчами.

3. **Маскирование**:  
   Вместо использования статического маскирования (как в BERT) применяется **динамическое маскирование**, где маски генерируются заново для каждой эпохи обучения. Это позволяет модели видеть больше разнообразных контекстов для каждого токена.

4. **Убрана задача NSP (Next Sentence Prediction)**:  
   В отличие от BERT, задача предсказания следующего предложения в RoBERTa исключена, так как она показала себя неэффективной. Вместо этого внимание сосредоточено исключительно на задаче маскированного языкового моделирования (Masked Language Modeling, MLM).

5. **Оптимизация гиперпараметров**:  
   RoBERTa использует тщательно подобранные гиперпараметры, такие как более высокие коэффициенты обучения и больший объем батчей, что улучшает качество предобучения.

---

#### Результаты
Благодаря этим изменениям RoBERTa достигает выдающихся результатов на многих NLP-бенчмарках, включая **GLUE**, **SuperGLUE**, **SQuAD**, **RACE** и других, превосходя оригинальный BERT и другие современные модели.

---

Оригинальная статья:  
**RoBERTa: A Robustly Optimized BERT Pretraining Approach**  
[https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692)

### Гиперпараметры
- **Максимальная длина токенов**: 128
- **Оптимизатор**: AdamW
- **Функция потерь**: `CrossEntropyLoss`
- **Размер батча**: 16
- **Коэффициент обучения**: $5 \times 10^{-5}$
- **Эпохи обучения**: 3
- **Warmup Steps**: 500
- **Весовая декомпозиция (Weight Decay)**: 0.01

Модель обучалась с использованием GPU для ускорения вычислений, при этом валидация выполнялась на тестовой выборке, где были достигнуты следующие результаты:
- **Точность (Accuracy)**: 93.43%
- **F1-мера**: 0.9343

### Оптимизатор AdamW
Оптимизатор **AdamW** (Adam с весовой декомпозицией) используется для улучшения стабильности обучения и предотвращения переобучения. Основное отличие от стандартного Adam — это добавление регуляризации (L2-регуляризация) через весовую декомпозицию, что делает обучение более устойчивым для глубоких моделей. Подробнее можно узнать из статьи: [Decoupled Weight Decay Regularization, ICLR 2019](https://arxiv.org/abs/1711.05101).

### Функция потерь
Для оптимизации обучения используется стандартная функция потерь `CrossEntropyLoss`, которая хорошо подходит для задач классификации, где классы представлены как one-hot метки. Эта функция вычисляет расстояние между предсказанными вероятностями и истинными метками. Подробности можно найти в документации PyTorch: [CrossEntropyLoss Documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).

  
Вот несколько альтернативных оптимизаторов, которые можно использовать вместо `AdamW`, а также их особенности и рекомендации:

### **1. Adam (torch.optim.Adam)**
**Описание:** Классический `Adam`, аналогичный `AdamW`, но без встроенного контроля весового затухания (`weight decay`). Можно использовать вместе с `weight_decay=0.01`, но `AdamW` считается более эффективным для трансформеров.

**Пример кода:**
```python
from torch.optim import Adam

optimizer = Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
```
**Когда использовать:** Если `AdamW` недоступен или если требуется контролировать `weight decay` вручную.

---

### **2. RMSprop (torch.optim.RMSprop)**
**Описание:** Оптимизатор, использующий экспоненциальное скользящее среднее градиентов. Может быть полезен для стабилизации обучения, но не так эффективен для трансформеров.

**Пример кода:**
```python
from torch.optim import RMSprop

optimizer = RMSprop(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY, alpha=0.9, momentum=0.9)
```
**Когда использовать:** Если обучение нестабильное, но `Adam` не помогает.

---

### **3. Adagrad (torch.optim.Adagrad)**
**Описание:** Подстраивает скорость обучения отдельно для каждого параметра. Хорош для разреженных данных, но может давать медленную сходимость.

**Пример кода:**
```python
from torch.optim import Adagrad

optimizer = Adagrad(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
```
**Когда использовать:** Если есть проблема с разреженными градиентами.

---

### **4. Adafactor (transformers.optimization.Adafactor)**
**Описание:** Альтернативный оптимизатор от Hugging Face, использующий адаптивные факторы. Экономит память, но требует правильной настройки `scale_parameter`.

**Пример кода:**
```python
from transformers import Adafactor

optimizer = Adafactor(
    model.parameters(), scale_parameter=False, relative_step=False, lr=LEARNING_RATE
)
```
**Когда использовать:** Если требуется уменьшить использование памяти.

---

### **5. Lion (lion_pytorch.Lion)**
**Описание:** Новый оптимизатор, представленный в 2023 году, использует моменты первого порядка, обеспечивая лучшую обобщаемость.

**Пример кода:**
```python
from lion_pytorch import Lion

optimizer = Lion(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
```
**Когда использовать:** Если хотите попробовать новый метод, который может быть более эффективным в некоторых случаях.

---

### **Рекомендации по выбору оптимизатора**
| **Оптимизатор** | **Лучше всего подходит для** | **Недостатки** |
|----------------|-------------------------|----------------|
| `AdamW` (по умолчанию) | Трансформеры, стандартное обучение | Требует ручной настройки LR |
| `Adam` | Если `AdamW` недоступен | Весовое затухание хуже реализовано |
| `RMSprop` | Нестабильное обучение | Может хуже работать для трансформеров |
| `Adagrad` | Разреженные данные | Замедляет обучение |
| `Adafactor` | Ограниченные вычислительные ресурсы | Требует тонкой настройки |
| `Lion` | Улучшенная обобщаемость | Требует тестирования |

