{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path('data/')\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_CACHE = Path('data/cache_dir/')\n",
    "DATA_CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_PATH_SAVE_MODELS = Path('data/models/')\n",
    "DATA_PATH_SAVE_MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_SYNTHETIC = Path('synthetic/')\n",
    "DATA_SYNTHETIC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [nanyy1025/covid_fake_news](https://huggingface.co/datasets/nanyy1025/covid_fake_news)\n",
    "\n",
    "Датасет `nanyy1025/covid_fake_news` на Hugging Face содержит 10 700 записей, каждая из которых представляет собой твит, связанный с COVID-19, с меткой \"real\" (реальный) или \"fake\" (фейковый). Датасет разделен на три части: обучающая выборка (6 420 записей), валидационная выборка (2 140 записей) и тестовая выборка (2 140 записей). Данные представлены в формате CSV и предназначены для задач классификации текста и zero-shot классификации. Датасет был использован в исследовании \"Fighting an Infodemic: COVID-19 Fake News Dataset\" (arXiv:2011.03327). ([Hugging Face](https://huggingface.co/datasets/nanyy1025/covid_fake_news?utm_source=chatgpt.com)) \n",
    "\n",
    "```bibtex\n",
    "@misc{patwa2020fighting,\n",
    "title={Fighting an Infodemic: COVID-19 Fake News Dataset}, \n",
    "author={Parth Patwa and Shivam Sharma and Srinivas PYKL and Vineeth Guptha and Gitanjali Kumari and Md Shad Akhtar and Asif Ekbal and Amitava Das and Tanmoy Chakraborty},\n",
    "year={2020},\n",
    "eprint={2011.03327},\n",
    "archivePrefix={arXiv},\n",
    "primaryClass={cs.CL}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_nanyy1025_dataset(dataset_name: str = \"nanyy1025/covid_fake_news\",\n",
    "                                     cache_dir=None) -> pd.DataFrame:\n",
    "    dataset = load_dataset(dataset_name, cache_dir=cache_dir)\n",
    "\n",
    "    train_df = dataset['train'].to_pandas()\n",
    "    test_df = dataset['test'].to_pandas()\n",
    "    validation_df = dataset['validation'].to_pandas()\n",
    "    merged_df: pd.DataFrame = pd.concat([train_df, test_df, validation_df], ignore_index=True)\n",
    "    merged_df.rename(columns={'tweet': 'text'}, inplace=True)\n",
    "    return merged_df\n",
    "\n",
    "nanyy1025_df = load_and_merge_nanyy1025_dataset( \"nanyy1025/covid_fake_news\", DATA_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanyy1025_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanyy1025_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [justinqbui/covid_fact_checked_google_api](https://huggingface.co/datasets/justinqbui/covid_fact_checked_google_api)\n",
    "\n",
    "Датасет **justinqbui/covid_fact_checked_google_api** с Hugging Face представляет собой выборку проверенных фактов, связанных с COVID-19, собранных с использованием Google Fact Checker API. Вот его основные характеристики:\n",
    "\n",
    "1. **Общий объем данных:**\n",
    "   - Содержит 3 043 записи.\n",
    "   - Первоначально было собрано 10 000 фактов, но для упрощения включены только те записи, где рейтинг был представлен одним словом — \"false\" (ложь) или \"true\" (правда). Около 90% фактов в датасете оценены как ложные.\n",
    "\n",
    "2. **Модальности и форматы:**\n",
    "   - Тип данных: текст.\n",
    "   - Формат: CSV.\n",
    "   - Также доступен в формате Parquet.\n",
    "\n",
    "3. **Описание данных:**\n",
    "   - Поля датасета:\n",
    "     - `text`: текст проверенного факта.\n",
    "     - `label`: метка правдивости (`true` или `false`).\n",
    "   - Аннотации созданы экспертами.\n",
    "   - Язык: английский (en-US).\n",
    "   - Датасет является монолингвальным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_justinqbui_dataset(dataset_name: str = \"justinqbui/covid_fact_checked_google_api\",\n",
    "                                     cache_dir=None) -> pd.DataFrame:\n",
    "    dataset = load_dataset(dataset_name, cache_dir=cache_dir)\n",
    "    return dataset['train'].to_pandas()\n",
    "\n",
    "justinqbui_1_df = load_and_merge_justinqbui_dataset( \"justinqbui/covid_fact_checked_google_api\", DATA_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justinqbui_1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justinqbui_1_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justinqbui_1_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [justinqbui/covid_fact_checked_polifact](https://huggingface.co/datasets/justinqbui/covid_fact_checked_polifact)\n",
    "\n",
    "Датасет **justinqbui/covid_fact_checked_polifact** с Hugging Face включает записи, связанные с проверкой фактов о COVID-19, собранных с помощью автоматического веб-скрейпера, который извлек данные из PolitiFact COVID Fact Checker. Вот его ключевые характеристики:\n",
    "\n",
    "1. **Общий объем данных:**\n",
    "   - Содержит 1 190 записей.\n",
    "   - Датасет состоит из утверждений и их оценки на правдивость.\n",
    "\n",
    "2. **Модальности и форматы:**\n",
    "   - Тип данных: текст.\n",
    "   - Формат: CSV.\n",
    "   - Также доступен в формате Parquet.\n",
    "\n",
    "3. **Описание данных:**\n",
    "   - Поля датасета:\n",
    "     - `claim`: текст утверждения.\n",
    "     - `rating`: оценка, присвоенная PolitiFact (7 значений: *half-true*, *full-flop*, *pants-fire*, *barely-true*, *true*, *mostly-true*, *false*).\n",
    "     - `adjusted_rating`: упрощенная версия оценки (3 значения: например, \"true\", \"false\" и промежуточные варианты).\n",
    "   - Поля предоставляют как детализированные, так и агрегированные оценки фактов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justinqbui_2_df = load_and_merge_justinqbui_dataset( \"justinqbui/covid_fact_checked_polifact\", DATA_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justinqbui_2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justinqbui_2_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justinqbui_2_df['rating'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justinqbui_2_df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justinqbui_2_df['adjusted rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Объединение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nanyy1025_df['source'] = 'nanyy1025'\n",
    "nanyy1025_df['original_label_1'] = nanyy1025_df['label']\n",
    "nanyy1025_df['label_idx'] = nanyy1025_df['label'].apply(lambda x: 1 if x == 'fake' else 0)\n",
    "nanyy1025_df = nanyy1025_df[['text', 'label_idx', 'original_label_1', 'source']]\n",
    "nanyy1025_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justinqbui_1_df['source'] = 'justinqbui_1'\n",
    "justinqbui_1_df['original_label_1'] = justinqbui_1_df['label']\n",
    "justinqbui_1_df['label_idx'] = justinqbui_1_df['label'].apply(lambda x: True if x == False else 0).astype(int)\n",
    "justinqbui_1_df = justinqbui_1_df[['text', 'label_idx', 'original_label_1', 'source']]\n",
    "justinqbui_1_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justinqbui_2_df['source'] = 'justinqbui_2'\n",
    "justinqbui_2_df['original_label_1'] = justinqbui_2_df['rating']\n",
    "justinqbui_2_df['original_label_2'] = justinqbui_2_df['adjusted rating']\n",
    "\n",
    "justinqbui_2_df['label_idx'] = justinqbui_2_df['adjusted rating'].apply(lambda x: 1 if x == 'false' else 0)\n",
    "\n",
    "justinqbui_2_df = justinqbui_2_df[['claim', 'label_idx', 'original_label_1', 'original_label_2', 'source']]\n",
    "justinqbui_2_df.rename(columns={'claim': 'text'}, inplace=True)\n",
    "justinqbui_2_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([nanyy1025_df, justinqbui_1_df, justinqbui_2_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['original_label_1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['label_idx'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Синтетика данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_file = DATA_SYNTHETIC / 'negative_text.txt'\n",
    "positive_file = DATA_SYNTHETIC / 'positive_text.txt'\n",
    "\n",
    "def create_dataframe_from_file(file_path: Path, label_idx: int) -> pd.DataFrame:\n",
    "    data = []\n",
    "    with file_path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                data.append({'text': line, 'label_idx': label_idx})\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "negative_df = create_dataframe_from_file(negative_file, label_idx=1)\n",
    "positive_df = create_dataframe_from_file(positive_file, label_idx=0)\n",
    "\n",
    "synthetic_df = pd.concat([negative_df, positive_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Комментарии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "def load_reddit_dataset(dataset_name=\"beenakurian/reddit_comments_subreddit_canada\", cache_dir=None) -> pd.DataFrame:\n",
    "    dataset = load_dataset(dataset_name, cache_dir=cache_dir)\n",
    "    df = dataset['train'].to_pandas()\n",
    "    df.rename(columns={'comment': 'text'}, inplace=True)    \n",
    "    df['original_label_1'] = df['sentiment']\n",
    "    df['label_idx'] = 2  \n",
    "    df['source'] = 'reddit_canada'\n",
    "    return df[['text', 'label_idx', 'original_label_1', 'source']]\n",
    "\n",
    "def load_toxic_comments_dataset(dataset_name=\"AiresPucrs/toxic-comments\", cache_dir=None) -> pd.DataFrame:\n",
    "    dataset = load_dataset(dataset_name, cache_dir=cache_dir)\n",
    "    df = dataset['train'].to_pandas()\n",
    "    df.rename(columns={'comment_text': 'text'}, inplace=True)\n",
    "    df['original_label_1'] = df['toxic']\n",
    "    df['label_idx'] = 2\n",
    "    df['source'] = 'toxic_comments'\n",
    "    return df[['text', 'label_idx', 'original_label_1', 'source']]\n",
    "\n",
    "def load_twitter_dataset(dataset_name=\"gxb912/large-twitter-tweets-sentiment\", cache_dir=None) -> pd.DataFrame:\n",
    "    dataset = load_dataset(dataset_name, cache_dir=cache_dir)\n",
    "    df_train = dataset['train'].to_pandas()\n",
    "    df_test = dataset['test'].to_pandas()\n",
    "    df = pd.concat([df_train, df_test], ignore_index=True)  # Исправлено объединение\n",
    "    df['original_label_1'] = df['sentiment']\n",
    "    df['label_idx'] = 2  \n",
    "    df['source'] = 'twitter'  # Исправлено название источника\n",
    "    return df[['text', 'label_idx', 'original_label_1', 'source']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [https://huggingface.co/datasets/gxb912/large-twitter-tweets-sentiment?row=62](https://huggingface.co/datasets/gxb912/large-twitter-tweets-sentiment?row=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = load_reddit_dataset(cache_dir=DATA_CACHE)\n",
    "toxic_df = load_toxic_comments_dataset(cache_dir=DATA_CACHE)\n",
    "twitter_df = load_twitter_dataset(cache_dir=DATA_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toxic_df_sample = toxic_df.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toxic_df_sample.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные размеченные руками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_and_process_data(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Загружает данные из Excel, добавляет дополнительные столбцы и возвращает обработанный DataFrame.\n",
    "    Преобразует label_idx в int, а если возникают ошибки - ставит значение 2.\n",
    "\n",
    "    :param file_path: Путь к файлу Excel.\n",
    "    :return: Обработанный DataFrame.\n",
    "    \"\"\"\n",
    "    data_marked_df = pd.read_excel(file_path).reset_index(drop=True)\n",
    "    \n",
    "    # Копируем new_label в label_idx\n",
    "    data_marked_df['label_idx'] = data_marked_df['new_label']\n",
    "    \n",
    "    # Пробуем привести к int, если ошибка (например, NaN или строка), ставим 2\n",
    "    data_marked_df['label_idx'] = pd.to_numeric(data_marked_df['label_idx'], errors='coerce').fillna(2).astype(int)\n",
    "    \n",
    "    # Добавляем источник\n",
    "    data_marked_df['source'] = 'handle_marked'\n",
    "\n",
    "    return data_marked_df[['text', 'label_idx', 'source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_marked_df = load_and_process_data(DATA_PATH / 'data_marked_df.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Общий датафрейм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_df['source'] = 'synthetic'\n",
    "final_df = pd.concat([combined_df, synthetic_df, reddit_df, toxic_df, twitter_df, data_marked_df], ignore_index=True)\n",
    "final_df[\"text\"] = final_df[\"text\"].replace(\"\", None).drop_duplicates().fillna(\"\")\n",
    "final_df.to_excel(DATA_PATH / 'covid_vaccine_fake.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2label = {\n",
    "    0: \"Real\",\n",
    "    1: \"Fake\",\n",
    "    2: \"Comments\"\n",
    "}\n",
    "\n",
    "final_df['label_str'] = final_df['label_idx'].map(idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['label_str'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # text = re.sub(r'#\\S+', '', text)  # Удаление хэштегов\n",
    "    text = re.sub(r'@\\S+', '', text)  # Удаление упоминаний\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)  # Удаление ссылок\n",
    "    return text.strip()  # Удаление лишних пробелов\n",
    "\n",
    "final_df['text'] = final_df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_excel(DATA_PATH / 'covid_vaccine_fake_clear.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
