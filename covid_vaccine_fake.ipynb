{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение определения фейковых фактов о COVID и вакцинации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path('data/')\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_CACHE = Path('data/cache_dir/')\n",
    "DATA_CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_PATH_SAVE_MODELS = Path('data/models/')\n",
    "DATA_PATH_SAVE_MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"roberta-base\"\n",
    "TRAIN_DF_NAME = \"covid_vaccine_fake_clear.xlsx\"\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_excel(DATA_PATH / TRAIN_DF_NAME)\n",
    "data_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def balance_label_idx(data_df: pd.DataFrame, target_label: int = 2, target_count: int = 30000, random_state: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Оставляет все категории label_idx, но уменьшает количество target_label до target_count случайным образом.\n",
    "    \n",
    "    :param data_df: Исходный DataFrame.\n",
    "    :param target_label: Значение label_idx, которое нужно уменьшить (по умолчанию 2).\n",
    "    :param target_count: Количество строк, которое нужно оставить для target_label (по умолчанию 30 000).\n",
    "    :param random_state: Фиксированный seed для воспроизводимости выборки.\n",
    "    :return: Обновленный DataFrame.\n",
    "    \"\"\"\n",
    "    # Оставляем все строки, кроме тех, у которых label_idx == target_label\n",
    "    df_other_labels = data_df[data_df[\"label_idx\"] != target_label]\n",
    "    \n",
    "    # Выбираем случайные target_count строк, где label_idx == target_label\n",
    "    df_target_label = data_df[data_df[\"label_idx\"] == target_label].sample(n=target_count, random_state=random_state)\n",
    "    \n",
    "    # Объединяем обратно\n",
    "    balanced_df = pd.concat([df_other_labels, df_target_label])\n",
    "\n",
    "    # Перемешиваем строки и сбрасываем индексы\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "# data_df = balance_label_idx(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.fillna(\"\")\n",
    "\n",
    "for col in data_df.select_dtypes(include=[\"object\", \"bool\"]).columns:\n",
    "    data_df[col] = data_df[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx = {key: int(idx) for idx, key in enumerate(data_df['label_str'].unique())}\n",
    "\n",
    "idx2label = dict([(v, k) for k, v in label2idx.items()])\n",
    "\n",
    "data_df['label_idx'] = data_df['label_str'].apply(lambda x: label2idx[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(idx2label)\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_df, test_df = train_test_split(data_df, test_size=0.1, stratify=data_df[\"label_idx\"], random_state=42, shuffle=True)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.1, stratify=train_val_df[\"label_idx\"], random_state=42, shuffle=True)\n",
    "\n",
    "print(f\"Размер тренировочного набора: {len(train_df)}\")\n",
    "print(f\"Размер валидационного набора: {len(val_df)}\")\n",
    "print(f\"Размер тестового набора: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import PreTrainedTokenizer\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataframe: pd.DataFrame, \n",
    "        tokenizer: PreTrainedTokenizer, \n",
    "        max_length: int, \n",
    "        tensor_dtype: Tuple[torch.dtype, torch.dtype, torch.dtype] = (torch.long, torch.long, torch.long)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Инициализация датасета с токенизацией.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame с колонками \"text\" и \"label\".\n",
    "            tokenizer (PreTrainedTokenizer): Токенайзер для преобразования текста.\n",
    "            max_length (int): Максимальная длина токенов.\n",
    "            tensor_dtype (tuple): Типы данных для токенов и меток.\n",
    "        \"\"\"\n",
    "        self.tensor_dtype = tensor_dtype\n",
    "\n",
    "        # Токенизация данных\n",
    "        tokenized_data = tokenizer(\n",
    "            dataframe[\"text\"].tolist(),\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        self.input_ids = tokenized_data[\"input_ids\"].to(dtype=self.tensor_dtype[0])\n",
    "        self.attention_mask = tokenized_data[\"attention_mask\"].to(dtype=self.tensor_dtype[1])\n",
    "        self.labels = torch.tensor(dataframe[\"label_idx\"].tolist(), dtype=self.tensor_dtype[2])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Возвращает количество примеров в датасете.\n",
    "        \"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Возвращает токенизированные данные и метки.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Индекс примера.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, torch.Tensor]: Словарь с токенами и меткой.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    dataframe: pd.DataFrame,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    max_length: int = 64,\n",
    "    batch_size: int = 16,\n",
    "    shuffle: bool = True,\n",
    "    tensor_dtype=(torch.long, torch.long, torch.long),\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Создание DataLoader из DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): DataFrame с колонками \"text\" и \"label\".\n",
    "        tokenizer (PreTrainedTokenizer): Токенайзер для преобразования текста.\n",
    "        max_length (int): Максимальная длина токенов.\n",
    "        batch_size (int): Размер батча.\n",
    "        shuffle (bool): Перемешивать ли данные.\n",
    "        tensor_dtype (tuple): Типы данных для токенов и меток.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: DataLoader для работы с моделью.\n",
    "    \"\"\"\n",
    "    dataset = TokenizedDataset(dataframe, tokenizer, max_length, tensor_dtype=tensor_dtype)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle, \n",
    "    )\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME, cache_dir=DATA_CACHE)\n",
    "\n",
    "train_loader = create_dataloader(\n",
    "    dataframe=train_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader(\n",
    "    dataframe=val_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = create_dataloader(\n",
    "    dataframe=test_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Выводим размер набора данных\n",
    "print(f\"Размер тренировочного набора: {len(train_loader.dataset)}\")\n",
    "print(f\"Размер валидационного набора: {len(val_loader.dataset)}\")\n",
    "print(f\"Размер тестового набора: {len(test_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Optimizer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "class VaccineFakeClassifierTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        optimizer: Optimizer,\n",
    "        criterion: torch.nn.Module,\n",
    "        device: torch.device,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Инициализация класса Trainer.\n",
    "\n",
    "        Args:\n",
    "            model: Модель для обучения (e.g., RobertaForSequenceClassification).\n",
    "            train_loader: DataLoader для обучающего набора.\n",
    "            val_loader: DataLoader для валидационного набора.\n",
    "            optimizer: Оптимизатор.\n",
    "            criterion: Функция потерь.\n",
    "            device: Устройство ('cuda' или 'cpu').\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.history = {\n",
    "            \"train_loss\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"train_metrics\": [],\n",
    "            \"val_metrics\": []\n",
    "        }\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train_epoch(self) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Обучение модели за одну эпоху.\n",
    "\n",
    "        Returns:\n",
    "            Средние потери и метрики за эпоху.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        for batch in tqdm(self.train_loader, desc=\"Training\"):\n",
    "            input_ids = batch[\"input_ids\"].to(self.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "            labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_preds.append(preds.cpu())\n",
    "\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        metrics = self._compute_metrics(all_preds, all_labels)\n",
    "\n",
    "        epoch_loss = running_loss / len(self.train_loader)\n",
    "        return epoch_loss, metrics\n",
    "\n",
    "    def validate_epoch(self) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Валидация модели за одну эпоху.\n",
    "\n",
    "        Returns:\n",
    "            Средние потери и метрики за эпоху.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc=\"Validation\"):\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                all_labels.append(labels.cpu())\n",
    "                all_preds.append(preds.cpu())\n",
    "\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        metrics = self._compute_metrics(all_preds, all_labels)\n",
    "\n",
    "        epoch_loss = running_loss / len(self.val_loader)\n",
    "        return epoch_loss, metrics\n",
    "\n",
    "    def fit(self, num_epochs: int):\n",
    "        \"\"\"\n",
    "        Обучение и валидация модели.\n",
    "\n",
    "        Args:\n",
    "            num_epochs: Общее количество эпох.\n",
    "        \"\"\"\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "            train_loss, train_metrics = self.train_epoch()\n",
    "            val_loss, val_metrics = self.validate_epoch()\n",
    "\n",
    "            self.history[\"train_loss\"].append(train_loss)\n",
    "            self.history[\"val_loss\"].append(val_loss)\n",
    "            self.history[\"train_metrics\"].append(train_metrics)\n",
    "            self.history[\"val_metrics\"].append(val_metrics)\n",
    "\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f}\")\n",
    "            print(f\"Train Metrics: {train_metrics} | Validation Metrics: {val_metrics}\")\n",
    "\n",
    "    def plot_results(self, metrics_to_plot=None):\n",
    "        \"\"\"\n",
    "        Построение графиков потерь и метрик для обучения и валидации.\n",
    "\n",
    "        Args:\n",
    "            metrics_to_plot: Список метрик для визуализации.\n",
    "        \"\"\"\n",
    "        if metrics_to_plot is None:\n",
    "            metrics_to_plot = [\"accuracy\", \"f1\"]\n",
    "\n",
    "        num_plots = len(metrics_to_plot) + 1\n",
    "        plt.figure(figsize=(15, 5 * (num_plots // 2 + 1)))\n",
    "\n",
    "        # График потерь\n",
    "        plt.subplot((num_plots + 1) // 2, 2, 1)\n",
    "        plt.plot(self.history[\"train_loss\"], label=\"Train Loss\", color=\"blue\", linestyle=\"--\")\n",
    "        plt.plot(self.history[\"val_loss\"], label=\"Validation Loss\", color=\"red\", linestyle=\"-\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss over Epochs\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Графики метрик\n",
    "        for i, metric in enumerate(metrics_to_plot, start=2):\n",
    "            train_metric = [m[metric] for m in self.history[\"train_metrics\"]]\n",
    "            val_metric = [m[metric] for m in self.history[\"val_metrics\"]]\n",
    "\n",
    "            plt.subplot((num_plots + 1) // 2, 2, i)\n",
    "            plt.plot(train_metric, label=f\"Train {metric.capitalize()}\", color=\"blue\", linestyle=\"--\")\n",
    "            plt.plot(val_metric, label=f\"Validation {metric.capitalize()}\", color=\"red\", linestyle=\"-\")\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(metric.capitalize())\n",
    "            plt.title(f\"{metric.capitalize()} over Epochs\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_metrics(preds: torch.Tensor, labels: torch.Tensor) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Вычисление метрик.\n",
    "\n",
    "        Args:\n",
    "            preds: Предсказания модели.\n",
    "            labels: Истинные метки.\n",
    "\n",
    "        Returns:\n",
    "            Словарь метрик.\n",
    "        \"\"\"\n",
    "        preds = preds.numpy()\n",
    "        labels = labels.numpy()\n",
    "\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(labels, preds),\n",
    "            \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
    "        }\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_EPOCHS = 5\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=NUM_CLASSES, cache_dir=DATA_CACHE)\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adagrad, RMSprop\n",
    "\n",
    "optimizer = Adagrad(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "optimizer = RMSprop(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY, alpha=0.9, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = VaccineFakeClassifierTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = DATA_PATH_SAVE_MODELS / \"covid_vaccine_fake_model\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"Модель и токинайзер сохранены в: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Энтропия (уверенность модели)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "def compute_entropy_thresholds(model: torch.nn.Module, dataloader: DataLoader, device: torch.device) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Вычисляет энтропию предсказаний модели для каждой категории и находит оптимальные пороги.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): Обученная модель для классификации.\n",
    "        dataloader (DataLoader): Валидационный датасет.\n",
    "        device (torch.device): CUDA или CPU.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Содержит оптимальные пороги энтропии, ROC AUC и DataFrame с результатами.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Словари для хранения энтропии и корректности предсказаний\n",
    "    entropies_per_class = defaultdict(list)\n",
    "    correct_per_class = defaultdict(list)\n",
    "\n",
    "    # Сбор данных\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probabilities = F.softmax(outputs.logits, dim=1)\n",
    "\n",
    "            # Энтропия предсказания\n",
    "            entropy = -torch.sum(probabilities * torch.log2(probabilities + 1e-15), dim=1)\n",
    "            predicted_labels = torch.argmax(probabilities, dim=1)\n",
    "            correct = (predicted_labels == labels).float()\n",
    "\n",
    "            # Сохраняем значения по классам\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                entropies_per_class[label].append(entropy[i].item())\n",
    "                correct_per_class[label].append(correct[i].item())\n",
    "\n",
    "    # Определение оптимального порога энтропии\n",
    "    optimal_thresholds = {}\n",
    "    roc_aucs = {}\n",
    "    all_entropies = []\n",
    "    all_correct = []\n",
    "    results = []\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for label, entropies in entropies_per_class.items():\n",
    "        correct = np.array(correct_per_class[label])\n",
    "        entropies = np.array(entropies)\n",
    "\n",
    "        if len(np.unique(correct)) < 2:\n",
    "            optimal_thresholds[label] = None\n",
    "            roc_aucs[label] = None\n",
    "            results.append({'label': label, 'AUC': None, 'optimal_threshold': None})\n",
    "            continue\n",
    "\n",
    "        # ROC и AUC\n",
    "        fpr, tpr, thresholds = roc_curve(correct, -entropies)\n",
    "        roc_auc = roc_auc_score(correct, -entropies)\n",
    "\n",
    "        # Оптимальный порог\n",
    "        youden_j = tpr - fpr\n",
    "        optimal_idx = np.argmax(youden_j)\n",
    "        optimal_threshold = -thresholds[optimal_idx]\n",
    "\n",
    "        optimal_thresholds[label] = optimal_threshold\n",
    "        roc_aucs[label] = roc_auc\n",
    "\n",
    "        all_entropies.extend(entropies)\n",
    "        all_correct.extend(correct)\n",
    "\n",
    "        # Сохранение результатов\n",
    "        results.append({'label': label, 'AUC': roc_auc, 'optimal_threshold': optimal_threshold})\n",
    "\n",
    "        # Добавляем кривую на график\n",
    "        plt.plot(fpr, tpr, label=f'Class {label} (AUC = {roc_auc:.4f})')\n",
    "        plt.scatter(fpr[optimal_idx], tpr[optimal_idx], color='red')\n",
    "\n",
    "    # Общий порог энтропии\n",
    "    all_entropies = np.array(all_entropies)\n",
    "    all_correct = np.array(all_correct)\n",
    "    fpr, tpr, thresholds = roc_curve(all_correct, -all_entropies)\n",
    "    roc_auc = roc_auc_score(all_correct, -all_entropies)\n",
    "    youden_j = tpr - fpr\n",
    "    optimal_idx = np.argmax(youden_j)\n",
    "    overall_optimal_threshold = -thresholds[optimal_idx]\n",
    "\n",
    "    print(f\"Общий оптимальный порог энтропии: {overall_optimal_threshold:.4f}\")\n",
    "    print(f\"AUC ROC (общий): {roc_auc:.4f}\")\n",
    "\n",
    "    # Добавляем общую ROC-кривую\n",
    "    plt.plot(fpr, tpr, label=f'Overall (AUC = {roc_auc:.4f})', linestyle='--', color='black')\n",
    "    plt.scatter(fpr[optimal_idx], tpr[optimal_idx], color='blue', label='Overall Optimal')\n",
    "\n",
    "    # Настройки графика\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for Each Class and Overall')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Создание DataFrame с результатами\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return {\n",
    "        'overall_optimal_threshold': overall_optimal_threshold,\n",
    "        'optimal_thresholds': optimal_thresholds,\n",
    "        'roc_aucs': roc_aucs,\n",
    "        'results_df': results_df\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_dict = compute_entropy_thresholds(model, val_loader, DEVICE)\n",
    "\n",
    "print(entropy_dict[\"results_df\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_dict[\"optimal_thresholds\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def test_model(\n",
    "    model: torch.nn.Module,\n",
    "    test_loader: DataLoader,\n",
    "    test_df: pd.DataFrame,\n",
    "    idx2label: Dict[int, str],\n",
    "    entropy_thresholds: Dict[int, float],  # Пороги энтропии по категориям\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Тестирует модель на тестовом DataLoader и возвращает DataFrame с результатами.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Обученная модель.\n",
    "        test_loader (DataLoader): DataLoader для тестового набора.\n",
    "        test_df (pd.DataFrame): Исходный DataFrame тестовых данных.\n",
    "        idx2label (dict): Словарь, отображающий индексы категорий в названия.\n",
    "        entropy_thresholds (dict): Оптимальные пороги энтропии по категориям.\n",
    "        device (torch.device): Устройство для вычислений (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame с результатами, содержащий:\n",
    "            - 'text': текст примера,\n",
    "            - 'true_label': истинная метка,\n",
    "            - 'predicted_label': предсказанная метка,\n",
    "            - 'probability': вероятность предсказания,\n",
    "            - 'correct': корректность предсказания (True/False),\n",
    "            - 'entropy': энтропия предсказания,\n",
    "            - 'entropy_threshold': порог энтропии для категории,\n",
    "            - 'passed_threshold': прошло ли предсказание порог энтропии.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    test_results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            true_labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            # Энтропия предсказания\n",
    "            entropy = -torch.sum(probabilities * torch.log2(probabilities + 1e-15), dim=1)\n",
    "\n",
    "            predictions = torch.argmax(probabilities, dim=-1).cpu().numpy()\n",
    "            true_labels = true_labels.cpu().numpy()\n",
    "            entropies = entropy.cpu().numpy()\n",
    "            probabilities = probabilities.cpu().numpy()\n",
    "\n",
    "            for true_label, predicted_label, probs, entropy_value in zip(true_labels, predictions, probabilities, entropies):\n",
    "                entropy_threshold = entropy_thresholds.get(predicted_label, None)\n",
    "                passed_threshold = entropy_value > entropy_threshold if entropy_threshold is not None else None\n",
    "\n",
    "                test_results.append({\n",
    "                    \"true_label\": idx2label[true_label],\n",
    "                    \"predicted_label\": idx2label[predicted_label],\n",
    "                    \"probability\": probs.tolist(),\n",
    "                    \"correct\": true_label == predicted_label,\n",
    "                    \"entropy\": entropy_value,\n",
    "                    \"entropy_threshold\": entropy_threshold,\n",
    "                    \"passed_threshold\": passed_threshold,\n",
    "                })\n",
    "\n",
    "    test_results_df = pd.DataFrame(test_results)\n",
    "\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "    test_df = pd.concat([test_df, test_results_df], axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(test_df[\"true_label\"], test_df[\"predicted_label\"])\n",
    "    f1 = f1_score(test_df[\"true_label\"], test_df[\"predicted_label\"], average=\"weighted\")\n",
    "\n",
    "    print(\"\\n=== Результаты тестирования ===\")\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(DATA_PATH_SAVE_MODELS / \"covid_vaccine_fake_model\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    DATA_PATH_SAVE_MODELS / \"covid_vaccine_fake_model\")\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df = test_model(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    test_df=test_df,\n",
    "    idx2label=idx2label,\n",
    "    entropy_thresholds=entropy_dict[\"optimal_thresholds\"],\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df.to_excel(DATA_PATH / 'test_results_learning.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
    "import torch\n",
    "\n",
    "\n",
    "def test_model_with_text(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    text: str,\n",
    "    max_length: int = 128,\n",
    "    label_map: dict = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Проверяет модель на введенном вручную тексте, используя argmax для классификации.\n",
    "\n",
    "    Args:\n",
    "        model (PreTrainedModel): Загруженная обученная модель (например, RobertaForSequenceClassification).\n",
    "        tokenizer (PreTrainedTokenizer): Токенизатор для подготовки текста.\n",
    "        text (str): Текст для классификации.\n",
    "        max_length (int): Максимальная длина токенизированного текста.\n",
    "        label_map (dict): Словарь для отображения меток, если метки числовые.\n",
    "\n",
    "    Returns:\n",
    "        dict: Словарь с текстом, вероятностями и предсказанными метками.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]  # Вероятности для каждого класса\n",
    "\n",
    "    predicted_index = torch.argmax(logits, dim=-1).item()\n",
    "    predicted_label = label_map[predicted_index] if label_map else predicted_index\n",
    "\n",
    "    # Результаты\n",
    "    result = {\n",
    "        \"text\": text,\n",
    "        \"probabilities\": probs.tolist(),\n",
    "        \"predicted_label\": predicted_label\n",
    "    }\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"COVID-19 vaccines do not affect fertility or reproductive health. This has been confirmed through multiple studies and ongoing research.\"\n",
    "\n",
    "result = test_model_with_text(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    text=text,\n",
    "    max_length=128,\n",
    "    label_map=idx2label\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"COVID-19 vaccines affect fertility or reproductive health.\"\n",
    "\n",
    "result = test_model_with_text(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    text=text,\n",
    "    max_length=128,\n",
    "    label_map=idx2label\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Arginine gave me a powerful boost of energy, I can really feel how it pumped me up. In the gym, it’s way easier to handle the weights, and my endurance is through the roof. It feels like my muscles are growing before my eyes, and the post-workout feeling is just awesome. I’m enjoying every moment as I feel my body reacting to the load. It’s definitely a great motivation to keep pushing forward! covid ruined everything though\"\n",
    "\n",
    "result = test_model_with_text(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    text=text,\n",
    "    max_length=128,\n",
    "    label_map=idx2label\n",
    ")\n",
    "\n",
    "print(result)\n",
    "\n",
    "text = '''\n",
    "The new strain of the Omicron coronavirus was first detected in South Africa and Botswana in November 2021. It is rapidly spreading all over the world. The omicron strain contains more than 30 mutations in the S-protein spike on the virus shell, with which it enters the cell.\n",
    "'''\n",
    "result = test_model_with_text(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    text=text,\n",
    "    max_length=128,\n",
    "    label_map=idx2label\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
