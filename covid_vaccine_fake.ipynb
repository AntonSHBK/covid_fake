{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–µ–π–∫–æ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤ –æ COVID –∏ –≤–∞–∫—Ü–∏–Ω–∞—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path('data/')\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_CACHE = Path('data/cache_dir/')\n",
    "DATA_CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_PATH_SAVE_MODELS = Path('data/models/')\n",
    "DATA_PATH_SAVE_MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_SYNTHETIC = Path('synthetic/')\n",
    "DATA_SYNTHETIC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Falah/News_Detection\"\n",
    "TRAIN_DF_NAME = \"covid_vaccine_fake_clear.xlsx\"\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –î–∞—Ç–∞—Å–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_excel(DATA_PATH / TRAIN_DF_NAME)\n",
    "data_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.rename(columns={'is_fake': 'label'}, inplace=True)\n",
    "data_df = data_df.fillna(\"\")\n",
    "\n",
    "for col in data_df.select_dtypes(include=[\"object\", \"bool\"]).columns:\n",
    "    data_df[col] = data_df[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_id2idx = {int(key): int(idx) for idx, key in enumerate(data_df['label_id'].unique())}\n",
    "\n",
    "# idx2label_id = dict([(v, k) for k, v in label_id2idx.items()])\n",
    "\n",
    "# idx2label = {k: df_messages[df_messages['label_id'] == v]['label'].iloc[0] for k, v in idx2label_id.items()}\n",
    "\n",
    "# label2idx = dict([(v, k) for k, v in idx2label.items()])\n",
    "\n",
    "# df_messages['label_idx'] = df_messages['label_id'].apply(lambda x: label_id2idx[x])\n",
    "\n",
    "# df_messages.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_df, test_df = train_test_split(data_df, test_size=0.1, stratify=data_df[\"label\"], random_state=42, shuffle=True)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.1, stratify=train_val_df[\"label\"], random_state=42, shuffle=True)\n",
    "\n",
    "print(f\"–†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞: {len(train_df)}\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞: {len(val_df)}\")\n",
    "print(f\"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ú–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=DATA_CACHE)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, cache_dir=DATA_CACHE)\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫:\", config.num_labels)\n",
    "print(\"–ò–º–µ–Ω–∞ –º–µ—Ç–æ–∫:\", config.id2label if hasattr(config, \"id2label\") else \"–ú–µ—Ç–∫–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_real = '''\n",
    "A fire engulfed a building in the Turkish ski resort of Kartalkaya in Bolu on Tuesday night. The kitchen staff tried to put out the fire for about 40 minutes. \n",
    "The flame entered the chimney through the hood, and soon reached the roof of the hotel. The kitchen staff ran out of the hotel, and the guests could not find out about the fire in time because the fire alarm went off.\n",
    "'''\n",
    "text_fake = \"Nuclear winter is near, the dogs have declared war on the cats and invaded their state.\"\n",
    "\n",
    "result_real = classifier(text_real)\n",
    "result_fake = classifier(text_fake)\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "print(\"–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\", result_real)\n",
    "print(\"–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Ñ–µ–π–∫–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\", result_fake)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "\n",
    "class VaccineFakeClassifierTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        num_labels: int,\n",
    "        train_dataset: Dataset,\n",
    "        val_dataset: Dataset,\n",
    "        test_dataset: Dataset,\n",
    "        cache_dir=DATA_CACHE,\n",
    "        output_dir=DATA_PATH_SAVE_MODELS,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=num_labels, cache_dir=cache_dir)\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "        for dataset, name in [\n",
    "            (train_dataset, \"train_dataset\"),\n",
    "            (val_dataset, \"val_dataset\"),\n",
    "            (test_dataset, \"test_dataset\"),\n",
    "        ]:\n",
    "            if \"text\" not in dataset.column_names or \"label\" not in dataset.column_names:\n",
    "                raise ValueError(f\"{name} –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏ 'text' –∏ 'label'.\")\n",
    "        \n",
    "        self.output_dir = output_dir\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "        self.tokenized_datasets = self._tokenize_datasets()\n",
    "        self.training_args = self._set_training_args()\n",
    "        self.trainer = self._initialize_trainer()\n",
    "\n",
    "    def _tokenize_datasets(self):\n",
    "        max_length = self.kwargs.get(\"max_length\", 64)\n",
    "        def preprocess_function(\n",
    "            examples: Dict[str, Any]\n",
    "        ) -> Dict[str, Any]:\n",
    "            return self.tokenizer(\n",
    "                examples[\"text\"],\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "            )\n",
    "\n",
    "        tokenized_datasets = self.train_dataset.map(preprocess_function, batched=True)\n",
    "        val_tokenized = self.val_dataset.map(preprocess_function, batched=True)\n",
    "        test_tokenized = self.test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "        return DatasetDict({\n",
    "            \"train\": tokenized_datasets,\n",
    "            \"validation\": val_tokenized,\n",
    "            \"test\": test_tokenized,\n",
    "        })\n",
    "\n",
    "    def _set_training_args(self):\n",
    "        return TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=self.kwargs.get(\"learning_rate\", 2e-5),\n",
    "            per_device_train_batch_size=self.kwargs.get(\"per_device_train_batch_size\", 16),\n",
    "            per_device_eval_batch_size=self.kwargs.get(\"per_device_eval_batch_size\", 16),\n",
    "            num_train_epochs=self.kwargs.get(\"num_train_epochs\", 5),\n",
    "            weight_decay=self.kwargs.get(\"weight_decay\", 0.01),\n",
    "            save_total_limit=self.kwargs.get(\"save_total_limit\", 2),\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=self.kwargs.get(\"metric_for_best_model\", \"accuracy\"),\n",
    "            logging_dir=self.kwargs.get(\"logging_dir\", \"./logs\"),\n",
    "            logging_steps=self.kwargs.get(\"logging_steps\", 10),\n",
    "        )\n",
    "\n",
    "    def _compute_metrics(self, eval_pred) -> Dict[str, float]:\n",
    "        logits, labels = eval_pred\n",
    "        predictions = logits.argmax(axis=-1)\n",
    "        return {\n",
    "            \"accuracy\": accuracy_score(labels, predictions),\n",
    "            \"f1\": f1_score(labels, predictions, average=\"weighted\"),\n",
    "        }\n",
    "\n",
    "    def _initialize_trainer(self) -> Trainer:\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "        return Trainer(\n",
    "            model=self.model,\n",
    "            args=self.training_args,\n",
    "            train_dataset=self.tokenized_datasets[\"train\"],\n",
    "            eval_dataset=self.tokenized_datasets[\"validation\"],\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=self._compute_metrics,\n",
    "        )\n",
    "\n",
    "    def train(self) -> None:\n",
    "        self.trainer.train()\n",
    "        self.model.save_pretrained(self.output_dir / 'vaccine_fake_model')\n",
    "        self.tokenizer.save_pretrained(self.output_dir / 'vaccine_fake_model')\n",
    "\n",
    "    def evaluate(self) -> Dict[str, float]:\n",
    "        return self.trainer.evaluate(self.tokenized_datasets[\"test\"])\n",
    "\n",
    "    def visualize_metrics(self, logs: Dict[str, Any]) -> None:\n",
    "\n",
    "        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫\n",
    "        epochs = list(range(1, len(logs[\"train_loss\"]) + 1))\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, logs[\"train_loss\"], label=\"Train Loss\")\n",
    "        plt.plot(epochs, logs[\"val_loss\"], label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Loss Over Epochs\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, logs[\"accuracy\"], label=\"Accuracy\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Accuracy Over Epochs\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    # \"evaluation_strategy\": \"epoch\",               # –û—Ü–µ–Ω–∫–∞ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏\n",
    "    \"learning_rate\": 5e-5,                        # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
    "    \"per_device_train_batch_size\": BATCH_SIZE,             # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏\n",
    "    \"per_device_eval_batch_size\": BATCH_SIZE,              # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "    \"num_train_epochs\": 1,                        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è\n",
    "    \"weight_decay\": 0.01,                         # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏             # –ó–∞–≥—Ä—É–∂–∞—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –≤ –∫–æ–Ω—Ü–µ\n",
    "    \"logging_dir\": DATA_PATH / \"logs\",            # –ü–∞–ø–∫–∞ –¥–ª—è –ª–æ–≥–æ–≤\n",
    "    \"max_length\": MAX_LENGTH,                            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = VaccineFakeClassifierTrainer(\n",
    "    model_name=\"roberta-base\",\n",
    "    num_labels=NUM_CLASSES,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    cache_dir=DATA_CACHE,\n",
    "    output_dir=DATA_PATH_SAVE_MODELS,\n",
    "    **training_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(\"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logs = trainer.trainer.state.log_history\n",
    "\n",
    "# train_loss = [log[\"loss\"] for log in logs if \"loss\" in log]\n",
    "# val_loss = [log[\"eval_loss\"] for log in logs if \"eval_loss\" in log]\n",
    "# accuracy = [log[\"eval_accuracy\"] for log in logs if \"eval_accuracy\" in log]\n",
    "\n",
    "# logs_to_visualize = {\n",
    "#     \"train_loss\": train_loss,\n",
    "#     \"val_loss\": val_loss,\n",
    "#     \"accuracy\": accuracy,\n",
    "# }\n",
    "\n",
    "# trainer.visualize_metrics(logs_to_visualize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "\n",
    "class VaccineFakeClassifier:\n",
    "    def __init__(self, model_path: str, idx2label: Dict[int, str]):\n",
    "        \"\"\"\n",
    "        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.\n",
    "        \n",
    "        :param model_path: –ü—É—Ç—å –∫ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.\n",
    "        :param idx2label: –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –º–µ—Ç–∫–∞–º–∏.\n",
    "        \"\"\"\n",
    "        self.tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "        self.model: RobertaForSequenceClassification = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "        self.idx2label = idx2label\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def predict_class(self, text: str) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∞ —Ç–µ–∫—Å—Ç–∞ –∏ –µ–≥–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏.\n",
    "        \n",
    "        :param text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç.\n",
    "        :return: –¢–µ–∫—Å—Ç–æ–≤–∞—è –º–µ—Ç–∫–∞ –∫–ª–∞—Å—Å–∞ –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å.\n",
    "        \"\"\"\n",
    "        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        # –ü–µ—Ä–µ–Ω–æ—Å —Ç–µ–Ω–∑–æ—Ä–æ–≤ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏\n",
    "        inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
    "        \n",
    "        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predicted_idx = logits.argmax(axis=-1).item()\n",
    "            score = logits.softmax(dim=-1).max().item()  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞\n",
    "        \n",
    "        return self.idx2label.get(predicted_idx, \"Unknown\"), score\n",
    "\n",
    "\n",
    "    def list_batch_predict(self, texts: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π.\n",
    "\n",
    "        :param texts: pandas Series —Å —Ç–µ–∫—Å—Ç–∞–º–∏.\n",
    "        :return: DataFrame —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏.\n",
    "        \"\"\"\n",
    "        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤\n",
    "        inputs = self.tokenizer(list(texts), return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        \n",
    "        # –ü–µ—Ä–µ–Ω–æ—Å —Ç–µ–Ω–∑–æ—Ä–æ–≤ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏\n",
    "        inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
    "        \n",
    "        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predicted_indices = logits.argmax(axis=-1).tolist()\n",
    "            scores = logits.softmax(dim=-1).max(dim=-1).values.tolist()  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
    "\n",
    "        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
    "        return pd.DataFrame({\n",
    "            \"predicted_label\": [self.idx2label.get(idx, \"Unknown\") for idx in predicted_indices],\n",
    "            \"score\": scores\n",
    "        })\n",
    "\n",
    "\n",
    "    def batch_predict(self, texts: pd.Series, batch_size: int = 16) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Å —É—á—ë—Ç–æ–º —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞.\n",
    "        \n",
    "        :param texts: pandas Series —Å —Ç–µ–∫—Å—Ç–∞–º–∏.\n",
    "        :param batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.\n",
    "        :return: DataFrame —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏ –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        scores = []\n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size].tolist()\n",
    "            inputs = self.tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "            inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                batch_predictions = logits.argmax(axis=-1).tolist()\n",
    "                batch_scores = logits.softmax(dim=-1).max(dim=-1).values.tolist()\n",
    "\n",
    "                predictions.extend(batch_predictions)\n",
    "                scores.extend(batch_scores)\n",
    "\n",
    "        return pd.DataFrame({\n",
    "            \"predicted_label\": [self.idx2label.get(idx, \"Unknown\") for idx in predictions],\n",
    "            \"score\": scores\n",
    "        })\n",
    "\n",
    "    def test_model(self, test_df: pd.DataFrame, batch_size: int = 16) -> Tuple[Dict[str, float], pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º DataFrame —Å —É—á—ë—Ç–æ–º –±–∞—Ç—á–µ–π.\n",
    "        \n",
    "        :param test_df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ 'text' –∏ 'label'.\n",
    "        :param batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.\n",
    "        :return: –ú–µ—Ç—Ä–∏–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ F1-–º–µ—Ä—ã, DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏.\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
    "        if \"text\" not in test_df.columns or \"label\" not in test_df.columns:\n",
    "            raise ValueError(\"DataFrame –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏ 'text' –∏ 'label'.\")\n",
    "\n",
    "        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "        predictions_df = self.batch_predict(test_df[\"text\"], batch_size=batch_size)\n",
    "\n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "        results_df = test_df.copy()\n",
    "        results_df[\"predicted_label\"] = predictions_df[\"predicted_label\"]\n",
    "        results_df[\"score\"] = predictions_df[\"score\"]\n",
    "        results_df[\"correct\"] = results_df[\"label\"] == results_df[\"predicted_label\"].map({v: k for k, v in self.idx2label.items()})\n",
    "\n",
    "        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫\n",
    "        accuracy = accuracy_score(results_df[\"label\"], results_df[\"predicted_label\"].map({v: k for k, v in self.idx2label.items()}))\n",
    "        f1 = f1_score(results_df[\"label\"], results_df[\"predicted_label\"].map({v: k for k, v in self.idx2label.items()}), average=\"weighted\")\n",
    "\n",
    "        return {\"accuracy\": accuracy, \"f1\": f1, \"results_df\": results_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = DATA_PATH_SAVE_MODELS / \"vaccine_fake_model\"\n",
    "\n",
    "idx2label = {\n",
    "    0: \"Real\",\n",
    "    1: \"Fake\"\n",
    "}\n",
    "\n",
    "classifier = VaccineFakeClassifier(model_path=model_path, idx2label=idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"About one in three adults used household cleaners and disinfectants unsafely to prevent such as using bleach on food products and improperly using household cleaners and disinfectants on hands or skin. Read more in.\"\n",
    "predicted_class = classifier.predict_class(text)\n",
    "print(f\"–ö–ª–∞—Å—Å —Ç–µ–∫—Å—Ç–∞: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤\n",
    "texts = [\n",
    "    \"About one in three adults used household cleaners and disinfectants unsafely to prevent such as using bleach on food products and improperly using household cleaners and disinfectants on hands or skin. Read more in\",\n",
    "    \"Drinking boiled hikaw-hikaw after doing suob/tuob or steam inhalation therapy will cure anyone with COVID-19\",\n",
    "    \"2nd week of Jan üá®üá≥ had mapped the genome &amp; shared it with WHO &amp; with wider üåç. We rapidly published a ‚Äúhow to‚Äù on building a PCR test for from our partner lab in üá©üá™. In the 3rd week WHO identified &amp; began contracting for validated production of these tests-\"\n",
    "]\n",
    "predicted_classes = classifier.list_batch_predict(texts)\n",
    "print(f\"–ö–ª–∞—Å—Å—ã —Ç–µ–∫—Å—Ç–æ–≤: {predicted_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = classifier.test_model(test_df)\n",
    "\n",
    "print(\"–ú–µ—Ç—Ä–∏–∫–∏:\", test_results['accuracy'],  test_results['f1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results['results_df'].sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
