{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение определения фейковых фактов о COVID и вакцинации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path('data/')\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_CACHE = Path('data/cache_dir/')\n",
    "DATA_CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_PATH_SAVE_MODELS = Path('data/models/')\n",
    "DATA_PATH_SAVE_MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_SYNTHETIC = Path('synthetic/')\n",
    "DATA_SYNTHETIC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"roberta-base\"\n",
    "TRAIN_DF_NAME = \"covid_vaccine_fake_clear.xlsx\"\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_excel(DATA_PATH / TRAIN_DF_NAME)\n",
    "data_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.rename(columns={'is_fake': 'label'}, inplace=True)\n",
    "data_df = data_df.fillna(\"\")\n",
    "\n",
    "for col in data_df.select_dtypes(include=[\"object\", \"bool\"]).columns:\n",
    "    data_df[col] = data_df[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_id2idx = {int(key): int(idx) for idx, key in enumerate(data_df['label_id'].unique())}\n",
    "\n",
    "# idx2label_id = dict([(v, k) for k, v in label_id2idx.items()])\n",
    "\n",
    "# idx2label = {k: df_messages[df_messages['label_id'] == v]['label'].iloc[0] for k, v in idx2label_id.items()}\n",
    "\n",
    "# label2idx = dict([(v, k) for k, v in idx2label.items()])\n",
    "\n",
    "# df_messages['label_idx'] = df_messages['label_id'].apply(lambda x: label_id2idx[x])\n",
    "\n",
    "# df_messages.head(1)\n",
    "\n",
    "\n",
    "idx2label = {\n",
    "    0: \"Real\",\n",
    "    1: \"Fake\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(idx2label)\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_df, test_df = train_test_split(data_df, test_size=0.1, stratify=data_df[\"label\"], random_state=42, shuffle=True)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.1, stratify=train_val_df[\"label\"], random_state=42, shuffle=True)\n",
    "\n",
    "print(f\"Размер тренировочного набора: {len(train_df)}\")\n",
    "print(f\"Размер валидационного набора: {len(val_df)}\")\n",
    "print(f\"Размер тестового набора: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import PreTrainedTokenizer\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataframe: pd.DataFrame, \n",
    "        tokenizer: PreTrainedTokenizer, \n",
    "        max_length: int, \n",
    "        tensor_dtype: Tuple[torch.dtype, torch.dtype, torch.dtype] = (torch.long, torch.long, torch.long)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Инициализация датасета с токенизацией.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame с колонками \"text\" и \"label\".\n",
    "            tokenizer (PreTrainedTokenizer): Токенайзер для преобразования текста.\n",
    "            max_length (int): Максимальная длина токенов.\n",
    "            tensor_dtype (tuple): Типы данных для токенов и меток.\n",
    "        \"\"\"\n",
    "        self.tensor_dtype = tensor_dtype\n",
    "\n",
    "        # Токенизация данных\n",
    "        tokenized_data = tokenizer(\n",
    "            dataframe[\"text\"].tolist(),\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        self.input_ids = tokenized_data[\"input_ids\"].to(dtype=self.tensor_dtype[0])\n",
    "        self.attention_mask = tokenized_data[\"attention_mask\"].to(dtype=self.tensor_dtype[1])\n",
    "        self.labels = torch.tensor(dataframe[\"label\"].tolist(), dtype=self.tensor_dtype[2])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Возвращает количество примеров в датасете.\n",
    "        \"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Возвращает токенизированные данные и метки.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Индекс примера.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, torch.Tensor]: Словарь с токенами и меткой.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import PreTrainedTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    dataframe: pd.DataFrame,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    max_length: int = 64,\n",
    "    batch_size: int = 16,\n",
    "    shuffle: bool = True,\n",
    "    tensor_dtype=(torch.long, torch.long, torch.long),\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Создание DataLoader из DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): DataFrame с колонками \"text\" и \"label\".\n",
    "        tokenizer (PreTrainedTokenizer): Токенайзер для преобразования текста.\n",
    "        max_length (int): Максимальная длина токенов.\n",
    "        batch_size (int): Размер батча.\n",
    "        shuffle (bool): Перемешивать ли данные.\n",
    "        tensor_dtype (tuple): Типы данных для токенов и меток.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: DataLoader для работы с моделью.\n",
    "    \"\"\"\n",
    "    dataset = TokenizedDataset(dataframe, tokenizer, max_length, tensor_dtype=tensor_dtype)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle, \n",
    "    )\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME, cache_dir=DATA_CACHE)\n",
    "\n",
    "train_loader = create_dataloader(\n",
    "    dataframe=train_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader(\n",
    "    dataframe=val_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = create_dataloader(\n",
    "    dataframe=test_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Выводим размер набора данных\n",
    "print(f\"Размер тренировочного набора: {len(train_loader.dataset)}\")\n",
    "print(f\"Размер валидационного набора: {len(val_loader.dataset)}\")\n",
    "print(f\"Размер тестового набора: {len(test_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Optimizer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "class VaccineFakeClassifierTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        optimizer: Optimizer,\n",
    "        criterion: torch.nn.Module,\n",
    "        device: torch.device,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Инициализация класса Trainer.\n",
    "\n",
    "        Args:\n",
    "            model: Модель для обучения (e.g., RobertaForSequenceClassification).\n",
    "            train_loader: DataLoader для обучающего набора.\n",
    "            val_loader: DataLoader для валидационного набора.\n",
    "            optimizer: Оптимизатор.\n",
    "            criterion: Функция потерь.\n",
    "            device: Устройство ('cuda' или 'cpu').\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.history = {\n",
    "            \"train_loss\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"train_metrics\": [],\n",
    "            \"val_metrics\": []\n",
    "        }\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train_epoch(self) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Обучение модели за одну эпоху.\n",
    "\n",
    "        Returns:\n",
    "            Средние потери и метрики за эпоху.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        for batch in tqdm(self.train_loader, desc=\"Training\"):\n",
    "            input_ids = batch[\"input_ids\"].to(self.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "            labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_preds.append(preds.cpu())\n",
    "\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        metrics = self._compute_metrics(all_preds, all_labels)\n",
    "\n",
    "        epoch_loss = running_loss / len(self.train_loader)\n",
    "        return epoch_loss, metrics\n",
    "\n",
    "    def validate_epoch(self) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Валидация модели за одну эпоху.\n",
    "\n",
    "        Returns:\n",
    "            Средние потери и метрики за эпоху.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc=\"Validation\"):\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                all_labels.append(labels.cpu())\n",
    "                all_preds.append(preds.cpu())\n",
    "\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        metrics = self._compute_metrics(all_preds, all_labels)\n",
    "\n",
    "        epoch_loss = running_loss / len(self.val_loader)\n",
    "        return epoch_loss, metrics\n",
    "\n",
    "    def fit(self, num_epochs: int):\n",
    "        \"\"\"\n",
    "        Обучение и валидация модели.\n",
    "\n",
    "        Args:\n",
    "            num_epochs: Общее количество эпох.\n",
    "        \"\"\"\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "            train_loss, train_metrics = self.train_epoch()\n",
    "            val_loss, val_metrics = self.validate_epoch()\n",
    "\n",
    "            self.history[\"train_loss\"].append(train_loss)\n",
    "            self.history[\"val_loss\"].append(val_loss)\n",
    "            self.history[\"train_metrics\"].append(train_metrics)\n",
    "            self.history[\"val_metrics\"].append(val_metrics)\n",
    "\n",
    "            print(f\"Train Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f}\")\n",
    "            print(f\"Train Metrics: {train_metrics} | Validation Metrics: {val_metrics}\")\n",
    "\n",
    "    def plot_results(self, metrics_to_plot=None):\n",
    "        \"\"\"\n",
    "        Построение графиков потерь и метрик для обучения и валидации.\n",
    "\n",
    "        Args:\n",
    "            metrics_to_plot: Список метрик для визуализации.\n",
    "        \"\"\"\n",
    "        if metrics_to_plot is None:\n",
    "            metrics_to_plot = [\"accuracy\", \"f1\"]\n",
    "\n",
    "        num_plots = len(metrics_to_plot) + 1\n",
    "        plt.figure(figsize=(15, 5 * (num_plots // 2 + 1)))\n",
    "\n",
    "        # График потерь\n",
    "        plt.subplot((num_plots + 1) // 2, 2, 1)\n",
    "        plt.plot(self.history[\"train_loss\"], label=\"Train Loss\", color=\"blue\", linestyle=\"--\")\n",
    "        plt.plot(self.history[\"val_loss\"], label=\"Validation Loss\", color=\"red\", linestyle=\"-\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss over Epochs\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Графики метрик\n",
    "        for i, metric in enumerate(metrics_to_plot, start=2):\n",
    "            train_metric = [m[metric] for m in self.history[\"train_metrics\"]]\n",
    "            val_metric = [m[metric] for m in self.history[\"val_metrics\"]]\n",
    "\n",
    "            plt.subplot((num_plots + 1) // 2, 2, i)\n",
    "            plt.plot(train_metric, label=f\"Train {metric.capitalize()}\", color=\"blue\", linestyle=\"--\")\n",
    "            plt.plot(val_metric, label=f\"Validation {metric.capitalize()}\", color=\"red\", linestyle=\"-\")\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(metric.capitalize())\n",
    "            plt.title(f\"{metric.capitalize()} over Epochs\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_metrics(preds: torch.Tensor, labels: torch.Tensor) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Вычисление метрик.\n",
    "\n",
    "        Args:\n",
    "            preds: Предсказания модели.\n",
    "            labels: Истинные метки.\n",
    "\n",
    "        Returns:\n",
    "            Словарь метрик.\n",
    "        \"\"\"\n",
    "        preds = preds.numpy()\n",
    "        labels = labels.numpy()\n",
    "\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(labels, preds),\n",
    "            \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
    "        }\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_EPOCHS = 5\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=NUM_CLASSES, cache_dir=DATA_CACHE)\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = VaccineFakeClassifierTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = DATA_PATH_SAVE_MODELS / \"covid_vaccine_fake_model\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"Модель и токинайзер сохранены в: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def test_model(\n",
    "    model: torch.nn.Module,\n",
    "    test_loader: DataLoader,\n",
    "    test_df: pd.DataFrame,\n",
    "    idx2label: dict,\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Тестирует модель на тестовом DataLoader и возвращает DataFrame с результатами.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Обученная модель.\n",
    "        test_loader (DataLoader): DataLoader для тестового набора.\n",
    "        test_df (pd.DataFrame): Исходный DataFrame тестовых данных.\n",
    "        idx2label (dict): Словарь, отображающий индексы категорий в названия.\n",
    "        device (torch.device): Устройство для вычислений (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame с результатами, содержащий:\n",
    "            - 'text': текст примера,\n",
    "            - 'true_label': истинная метка,\n",
    "            - 'predicted_label': предсказанная метка,\n",
    "            - 'probability': вероятность предсказания,\n",
    "            - 'correct': корректность предсказания (True/False).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    test_results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            true_labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            probabilities = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            true_labels = true_labels.cpu().numpy()\n",
    "\n",
    "            for true_label, predicted_label, probs in zip(true_labels, predictions, probabilities):\n",
    "                test_results.append({\n",
    "                    \"true_label\": idx2label[true_label],\n",
    "                    \"predicted_label\": idx2label[predicted_label],\n",
    "                    \"probability\": probs.tolist(),\n",
    "                    \"correct\": true_label == predicted_label,\n",
    "                })\n",
    "\n",
    "    test_results_df = pd.DataFrame(test_results)\n",
    "\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "    test_df = pd.concat([test_df, test_results_df], axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(test_df[\"true_label\"], test_df[\"predicted_label\"])\n",
    "    f1 = f1_score(test_df[\"true_label\"], test_df[\"predicted_label\"], average=\"weighted\")\n",
    "\n",
    "    print(\"\\n=== Результаты тестирования ===\")\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df = test_model(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    test_df=test_df,\n",
    "    idx2label=idx2label,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
    "import torch\n",
    "\n",
    "\n",
    "def test_model_with_text(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    text: str,\n",
    "    max_length: int = 128,\n",
    "    label_map: dict = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Проверяет модель на введенном вручную тексте, используя argmax для классификации.\n",
    "\n",
    "    Args:\n",
    "        model (PreTrainedModel): Загруженная обученная модель (например, RobertaForSequenceClassification).\n",
    "        tokenizer (PreTrainedTokenizer): Токенизатор для подготовки текста.\n",
    "        text (str): Текст для классификации.\n",
    "        max_length (int): Максимальная длина токенизированного текста.\n",
    "        label_map (dict): Словарь для отображения меток, если метки числовые.\n",
    "\n",
    "    Returns:\n",
    "        dict: Словарь с текстом, вероятностями и предсказанными метками.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]  # Вероятности для каждого класса\n",
    "\n",
    "    predicted_index = torch.argmax(logits, dim=-1).item()\n",
    "    predicted_label = label_map[predicted_index] if label_map else predicted_index\n",
    "\n",
    "    # Результаты\n",
    "    result = {\n",
    "        \"text\": text,\n",
    "        \"probabilities\": probs.tolist(),\n",
    "        \"predicted_label\": predicted_label\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"COVID-19 vaccines do not affect fertility or reproductive health. This has been confirmed through multiple studies and ongoing research.\"\n",
    "\n",
    "result = test_model_with_text(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    text=text,\n",
    "    max_length=128,\n",
    "    label_map=idx2label\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"COVID-19 vaccines affect fertility or reproductive health.\"\n",
    "\n",
    "result = test_model_with_text(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    text=text,\n",
    "    max_length=128,\n",
    "    label_map=idx2label\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
