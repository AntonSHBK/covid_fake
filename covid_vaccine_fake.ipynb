{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение определения фейковых фактов о COVID и вакцинации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path('data/')\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_CACHE = Path('data/cache_dir/')\n",
    "DATA_CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_PATH_SAVE_MODELS = Path('data/models/')\n",
    "DATA_PATH_SAVE_MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_SYNTHETIC = Path('synthetic/')\n",
    "DATA_SYNTHETIC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Falah/News_Detection\"\n",
    "TRAIN_DF_NAME = \"covid_vaccine_fake_clear.xlsx\"\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_excel(DATA_PATH / TRAIN_DF_NAME)\n",
    "data_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.rename(columns={'is_fake': 'label'}, inplace=True)\n",
    "data_df = data_df.fillna(\"\")\n",
    "\n",
    "for col in data_df.select_dtypes(include=[\"object\", \"bool\"]).columns:\n",
    "    data_df[col] = data_df[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_id2idx = {int(key): int(idx) for idx, key in enumerate(data_df['label_id'].unique())}\n",
    "\n",
    "# idx2label_id = dict([(v, k) for k, v in label_id2idx.items()])\n",
    "\n",
    "# idx2label = {k: df_messages[df_messages['label_id'] == v]['label'].iloc[0] for k, v in idx2label_id.items()}\n",
    "\n",
    "# label2idx = dict([(v, k) for k, v in idx2label.items()])\n",
    "\n",
    "# df_messages['label_idx'] = df_messages['label_id'].apply(lambda x: label_id2idx[x])\n",
    "\n",
    "# df_messages.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_df, test_df = train_test_split(data_df, test_size=0.1, stratify=data_df[\"label\"], random_state=42, shuffle=True)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.1, stratify=train_val_df[\"label\"], random_state=42, shuffle=True)\n",
    "\n",
    "print(f\"Размер тренировочного набора: {len(train_df)}\")\n",
    "print(f\"Размер валидационного набора: {len(val_df)}\")\n",
    "print(f\"Размер тестового набора: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример использования исходной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=DATA_CACHE)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, cache_dir=DATA_CACHE)\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"Количество меток:\", config.num_labels)\n",
    "print(\"Имена меток:\", config.id2label if hasattr(config, \"id2label\") else \"Метки не указаны\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_real = '''\n",
    "A fire engulfed a building in the Turkish ski resort of Kartalkaya in Bolu on Tuesday night. The kitchen staff tried to put out the fire for about 40 minutes. \n",
    "The flame entered the chimney through the hood, and soon reached the roof of the hotel. The kitchen staff ran out of the hotel, and the guests could not find out about the fire in time because the fire alarm went off.\n",
    "'''\n",
    "text_fake = \"Nuclear winter is near, the dogs have declared war on the cats and invaded their state.\"\n",
    "\n",
    "result_real = classifier(text_real)\n",
    "result_fake = classifier(text_fake)\n",
    "\n",
    "# Выводим результаты\n",
    "print(\"Результат для реального текста:\", result_real)\n",
    "print(\"Результат для фейкового текста:\", result_fake)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дообучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "\n",
    "class VaccineFakeClassifierTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        num_labels: int,\n",
    "        train_dataset: Dataset,\n",
    "        val_dataset: Dataset,\n",
    "        test_dataset: Dataset,\n",
    "        cache_dir=DATA_CACHE,\n",
    "        output_dir=DATA_PATH_SAVE_MODELS,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=num_labels, cache_dir=cache_dir)\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "        for dataset, name in [\n",
    "            (train_dataset, \"train_dataset\"),\n",
    "            (val_dataset, \"val_dataset\"),\n",
    "            (test_dataset, \"test_dataset\"),\n",
    "        ]:\n",
    "            if \"text\" not in dataset.column_names or \"label\" not in dataset.column_names:\n",
    "                raise ValueError(f\"{name} должен содержать колонки 'text' и 'label'.\")\n",
    "        \n",
    "        self.output_dir = output_dir\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "        self.tokenized_datasets = self._tokenize_datasets()\n",
    "        self.training_args = self._set_training_args()\n",
    "        self.trainer = self._initialize_trainer()\n",
    "\n",
    "    def _tokenize_datasets(self):\n",
    "        max_length = self.kwargs.get(\"max_length\", 64)\n",
    "        def preprocess_function(\n",
    "            examples: Dict[str, Any]\n",
    "        ) -> Dict[str, Any]:\n",
    "            return self.tokenizer(\n",
    "                examples[\"text\"],\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "            )\n",
    "\n",
    "        tokenized_datasets = self.train_dataset.map(preprocess_function, batched=True)\n",
    "        val_tokenized = self.val_dataset.map(preprocess_function, batched=True)\n",
    "        test_tokenized = self.test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "        return DatasetDict({\n",
    "            \"train\": tokenized_datasets,\n",
    "            \"validation\": val_tokenized,\n",
    "            \"test\": test_tokenized,\n",
    "        })\n",
    "\n",
    "    def _set_training_args(self):\n",
    "        return TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=self.kwargs.get(\"learning_rate\", 2e-5),\n",
    "            per_device_train_batch_size=self.kwargs.get(\"per_device_train_batch_size\", 16),\n",
    "            per_device_eval_batch_size=self.kwargs.get(\"per_device_eval_batch_size\", 16),\n",
    "            num_train_epochs=self.kwargs.get(\"num_train_epochs\", 5),\n",
    "            weight_decay=self.kwargs.get(\"weight_decay\", 0.01),\n",
    "            save_total_limit=self.kwargs.get(\"save_total_limit\", 2),\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=self.kwargs.get(\"metric_for_best_model\", \"accuracy\"),\n",
    "            logging_dir=self.kwargs.get(\"logging_dir\", \"./logs\"),\n",
    "            logging_steps=self.kwargs.get(\"logging_steps\", 10),\n",
    "        )\n",
    "\n",
    "    def _compute_metrics(self, eval_pred) -> Dict[str, float]:\n",
    "        logits, labels = eval_pred\n",
    "        predictions = logits.argmax(axis=-1)\n",
    "        return {\n",
    "            \"accuracy\": accuracy_score(labels, predictions),\n",
    "            \"f1\": f1_score(labels, predictions, average=\"weighted\"),\n",
    "        }\n",
    "\n",
    "    def _initialize_trainer(self) -> Trainer:\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "        return Trainer(\n",
    "            model=self.model,\n",
    "            args=self.training_args,\n",
    "            train_dataset=self.tokenized_datasets[\"train\"],\n",
    "            eval_dataset=self.tokenized_datasets[\"validation\"],\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=self._compute_metrics,\n",
    "        )\n",
    "\n",
    "    def train(self) -> None:\n",
    "        self.trainer.train()\n",
    "        self.model.save_pretrained(self.output_dir / 'vaccine_fake_model')\n",
    "        self.tokenizer.save_pretrained(self.output_dir / 'vaccine_fake_model')\n",
    "\n",
    "    def evaluate(self) -> Dict[str, float]:\n",
    "        return self.trainer.evaluate(self.tokenized_datasets[\"test\"])\n",
    "\n",
    "    def visualize_metrics(self, logs: Dict[str, Any]) -> None:\n",
    "\n",
    "        # Визуализация метрик\n",
    "        epochs = list(range(1, len(logs[\"train_loss\"]) + 1))\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, logs[\"train_loss\"], label=\"Train Loss\")\n",
    "        plt.plot(epochs, logs[\"val_loss\"], label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Loss Over Epochs\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, logs[\"accuracy\"], label=\"Accuracy\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Accuracy Over Epochs\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    \"evaluation_strategy\": \"epoch\",               # Оценка после каждой эпохи\n",
    "    \"learning_rate\": 5e-5,                        # Скорость обучения\n",
    "    \"per_device_train_batch_size\": BATCH_SIZE,             # Размер батча для тренировки\n",
    "    \"per_device_eval_batch_size\": BATCH_SIZE,              # Размер батча для валидации\n",
    "    \"num_train_epochs\": 1,                        # Количество эпох обучения\n",
    "    \"weight_decay\": 0.01,                         # Коэффициент регуляризации\n",
    "    \"save_total_limit\": 2,                        # Максимум сохранённых моделей\n",
    "    \"load_best_model_at_end\": True,               # Загружать лучшую модель в конце\n",
    "    \"metric_for_best_model\": \"accuracy\",          # Метрика для выбора лучшей модели\n",
    "    \"logging_dir\": DATA_PATH / \"logs\",            # Папка для логов\n",
    "    \"logging_steps\": 10,                          # Шаги для логирования\n",
    "    \"max_length\": MAX_LENGTH,                            # Максимальная длина токенизированного текста\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = VaccineFakeClassifierTrainer(\n",
    "    model_name=\"roberta-base\",\n",
    "    num_labels=NUM_CLASSES,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    cache_dir=DATA_CACHE,\n",
    "    output_dir=DATA_PATH_SAVE_MODELS,\n",
    "    **training_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(\"Результаты на тестовом наборе:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = trainer.trainer.state.log_history\n",
    "\n",
    "train_loss = [log[\"loss\"] for log in logs if \"loss\" in log]\n",
    "val_loss = [log[\"eval_loss\"] for log in logs if \"eval_loss\" in log]\n",
    "accuracy = [log[\"eval_accuracy\"] for log in logs if \"eval_accuracy\" in log]\n",
    "\n",
    "logs_to_visualize = {\n",
    "    \"train_loss\": train_loss,\n",
    "    \"val_loss\": val_loss,\n",
    "    \"accuracy\": accuracy,\n",
    "}\n",
    "\n",
    "trainer.visualize_metrics(logs_to_visualize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "\n",
    "class VaccineFakeClassifier:\n",
    "    def __init__(self, model_path: str, idx2label: Dict[int, str]):\n",
    "        \"\"\"\n",
    "        Инициализация модели и токенайзера для тестирования.\n",
    "        \n",
    "        :param model_path: Путь к дообученной модели.\n",
    "        :param idx2label: Словарь для сопоставления индексов классов с текстовыми метками.\n",
    "        \"\"\"\n",
    "        self.tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "        self.model: RobertaForSequenceClassification = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "        self.idx2label = idx2label\n",
    "\n",
    "    def predict_class(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Определение класса текста.\n",
    "        \n",
    "        :param text: Входной текст.\n",
    "        :return: Текстовая метка класса.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        outputs = self.model(**inputs)\n",
    "        predicted_idx = outputs.logits.argmax(axis=-1).item()\n",
    "        return self.idx2label.get(predicted_idx, \"Unknown\")\n",
    "\n",
    "    def batch_predict(self, texts: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Определение классов для списка текстов.\n",
    "        \n",
    "        :param texts: Список входных текстов.\n",
    "        :return: Список текстовых меток классов.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        outputs = self.model(**inputs)\n",
    "        predicted_indices = outputs.logits.argmax(axis=-1).tolist()\n",
    "        return [self.idx2label.get(idx, \"Unknown\") for idx in predicted_indices]\n",
    "\n",
    "    def test_model(self, texts: List[str], labels: List[int]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Тестирование модели на наборе данных.\n",
    "        \n",
    "        :param texts: Список входных текстов.\n",
    "        :param labels: Список истинных меток.\n",
    "        :return: Метрики точности и F1-меры.\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        outputs = self.model(**inputs)\n",
    "        predicted_indices = outputs.logits.argmax(axis=-1).tolist()\n",
    "\n",
    "        accuracy = accuracy_score(labels, predicted_indices)\n",
    "        f1 = f1_score(labels, predicted_indices, average=\"weighted\")\n",
    "\n",
    "        return {\"accuracy\": accuracy, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Путь к дообученной модели\n",
    "model_path = DATA_PATH_SAVE_MODELS / \"/vaccine_fake_model\"\n",
    "\n",
    "# Словарь сопоставления индексов классов и текстовых меток\n",
    "idx2label = {\n",
    "    0: \"Real\",\n",
    "    1: \"Fake\"\n",
    "}\n",
    "\n",
    "classifier = VaccineFakeClassifier(model_path=model_path, idx2label=idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение класса для одного текста\n",
    "text = \"The COVID-19 vaccine is effective and saves lives.\"\n",
    "predicted_class = classifier.predict_class(text)\n",
    "print(f\"Класс текста: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение классов для нескольких текстов\n",
    "texts = [\n",
    "    \"The COVID-19 vaccine is effective and saves lives.\",\n",
    "    \"Vaccines are a conspiracy to control the population.\"\n",
    "]\n",
    "predicted_classes = classifier.batch_predict(texts)\n",
    "print(f\"Классы текстов: {predicted_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные для тестирования\n",
    "test_texts = [\n",
    "    \"The COVID-19 vaccine is effective.\",\n",
    "    \"COVID-19 is a hoax created by the government.\"\n",
    "]\n",
    "test_labels = [1, 0]  # 1 - Real, 0 - Fake\n",
    "\n",
    "# Тестирование модели\n",
    "metrics = classifier.test_model(test_texts, test_labels)\n",
    "print(f\"Метрики тестирования: {metrics}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
