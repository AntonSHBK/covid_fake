{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение определения фейковых фактов о COVID и вакцинации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path('data/')\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_CACHE = Path('data/cache_dir/')\n",
    "DATA_CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_PATH_SAVE_MODELS = Path('data/models/')\n",
    "DATA_PATH_SAVE_MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_SYNTHETIC = Path('synthetic/')\n",
    "DATA_SYNTHETIC.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Falah/News_Detection\"\n",
    "TRAIN_DF_NAME = \"covid_vaccine_fake_clear.xlsx\"\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_excel(DATA_PATH / TRAIN_DF_NAME)\n",
    "data_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.rename(columns={'is_fake': 'label'}, inplace=True)\n",
    "data_df = data_df.fillna(\"\")\n",
    "\n",
    "for col in data_df.select_dtypes(include=[\"object\", \"bool\"]).columns:\n",
    "    data_df[col] = data_df[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_id2idx = {int(key): int(idx) for idx, key in enumerate(data_df['label_id'].unique())}\n",
    "\n",
    "# idx2label_id = dict([(v, k) for k, v in label_id2idx.items()])\n",
    "\n",
    "# idx2label = {k: df_messages[df_messages['label_id'] == v]['label'].iloc[0] for k, v in idx2label_id.items()}\n",
    "\n",
    "# label2idx = dict([(v, k) for k, v in idx2label.items()])\n",
    "\n",
    "# df_messages['label_idx'] = df_messages['label_id'].apply(lambda x: label_id2idx[x])\n",
    "\n",
    "# df_messages.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_df, test_df = train_test_split(data_df, test_size=0.1, stratify=data_df[\"label\"], random_state=42, shuffle=True)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.1, stratify=train_val_df[\"label\"], random_state=42, shuffle=True)\n",
    "\n",
    "print(f\"Размер тренировочного набора: {len(train_df)}\")\n",
    "print(f\"Размер валидационного набора: {len(val_df)}\")\n",
    "print(f\"Размер тестового набора: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример использования исходной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=DATA_CACHE)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, cache_dir=DATA_CACHE)\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"Количество меток:\", config.num_labels)\n",
    "print(\"Имена меток:\", config.id2label if hasattr(config, \"id2label\") else \"Метки не указаны\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_real = '''\n",
    "A fire engulfed a building in the Turkish ski resort of Kartalkaya in Bolu on Tuesday night. The kitchen staff tried to put out the fire for about 40 minutes. \n",
    "The flame entered the chimney through the hood, and soon reached the roof of the hotel. The kitchen staff ran out of the hotel, and the guests could not find out about the fire in time because the fire alarm went off.\n",
    "'''\n",
    "text_fake = \"Nuclear winter is near, the dogs have declared war on the cats and invaded their state.\"\n",
    "\n",
    "result_real = classifier(text_real)\n",
    "result_fake = classifier(text_fake)\n",
    "\n",
    "# Выводим результаты\n",
    "print(\"Результат для реального текста:\", result_real)\n",
    "print(\"Результат для фейкового текста:\", result_fake)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дообучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "\n",
    "class VaccineFakeClassifierTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        num_labels: int,\n",
    "        train_dataset: Dataset,\n",
    "        val_dataset: Dataset,\n",
    "        test_dataset: Dataset,\n",
    "        cache_dir=DATA_CACHE,\n",
    "        output_dir=DATA_PATH_SAVE_MODELS,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=num_labels, cache_dir=cache_dir)\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "        for dataset, name in [\n",
    "            (train_dataset, \"train_dataset\"),\n",
    "            (val_dataset, \"val_dataset\"),\n",
    "            (test_dataset, \"test_dataset\"),\n",
    "        ]:\n",
    "            if \"text\" not in dataset.column_names or \"label\" not in dataset.column_names:\n",
    "                raise ValueError(f\"{name} должен содержать колонки 'text' и 'label'.\")\n",
    "        \n",
    "        self.output_dir = output_dir\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "        self.tokenized_datasets = self._tokenize_datasets()\n",
    "        self.training_args = self._set_training_args()\n",
    "        self.trainer = self._initialize_trainer()\n",
    "\n",
    "    def _tokenize_datasets(self):\n",
    "        max_length = self.kwargs.get(\"max_length\", 64)\n",
    "        def preprocess_function(\n",
    "            examples: Dict[str, Any]\n",
    "        ) -> Dict[str, Any]:\n",
    "            return self.tokenizer(\n",
    "                examples[\"text\"],\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "            )\n",
    "\n",
    "        tokenized_datasets = self.train_dataset.map(preprocess_function, batched=True)\n",
    "        val_tokenized = self.val_dataset.map(preprocess_function, batched=True)\n",
    "        test_tokenized = self.test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "        return DatasetDict({\n",
    "            \"train\": tokenized_datasets,\n",
    "            \"validation\": val_tokenized,\n",
    "            \"test\": test_tokenized,\n",
    "        })\n",
    "\n",
    "    def _set_training_args(self):\n",
    "        return TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=self.kwargs.get(\"learning_rate\", 2e-5),\n",
    "            per_device_train_batch_size=self.kwargs.get(\"per_device_train_batch_size\", 16),\n",
    "            per_device_eval_batch_size=self.kwargs.get(\"per_device_eval_batch_size\", 16),\n",
    "            num_train_epochs=self.kwargs.get(\"num_train_epochs\", 5),\n",
    "            weight_decay=self.kwargs.get(\"weight_decay\", 0.01),\n",
    "            save_total_limit=self.kwargs.get(\"save_total_limit\", 2),\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=self.kwargs.get(\"metric_for_best_model\", \"accuracy\"),\n",
    "            logging_dir=self.kwargs.get(\"logging_dir\", \"./logs\"),\n",
    "            logging_steps=self.kwargs.get(\"logging_steps\", 10),\n",
    "        )\n",
    "\n",
    "    def _compute_metrics(self, eval_pred) -> Dict[str, float]:\n",
    "        logits, labels = eval_pred\n",
    "        predictions = logits.argmax(axis=-1)\n",
    "        return {\n",
    "            \"accuracy\": accuracy_score(labels, predictions),\n",
    "            \"f1\": f1_score(labels, predictions, average=\"weighted\"),\n",
    "        }\n",
    "\n",
    "    def _initialize_trainer(self) -> Trainer:\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "        return Trainer(\n",
    "            model=self.model,\n",
    "            args=self.training_args,\n",
    "            train_dataset=self.tokenized_datasets[\"train\"],\n",
    "            eval_dataset=self.tokenized_datasets[\"validation\"],\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=self._compute_metrics,\n",
    "        )\n",
    "\n",
    "    def train(self) -> None:\n",
    "        self.trainer.train()\n",
    "        self.model.save_pretrained(self.output_dir / 'vaccine_fake_model')\n",
    "        self.tokenizer.save_pretrained(self.output_dir / 'vaccine_fake_model')\n",
    "\n",
    "    def evaluate(self) -> Dict[str, float]:\n",
    "        return self.trainer.evaluate(self.tokenized_datasets[\"test\"])\n",
    "\n",
    "    def visualize_metrics(self, logs: Dict[str, Any]) -> None:\n",
    "\n",
    "        # Визуализация метрик\n",
    "        epochs = list(range(1, len(logs[\"train_loss\"]) + 1))\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, logs[\"train_loss\"], label=\"Train Loss\")\n",
    "        plt.plot(epochs, logs[\"val_loss\"], label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Loss Over Epochs\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, logs[\"accuracy\"], label=\"Accuracy\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Accuracy Over Epochs\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    # \"evaluation_strategy\": \"epoch\",               # Оценка после каждой эпохи\n",
    "    \"learning_rate\": 5e-5,                        # Скорость обучения\n",
    "    \"per_device_train_batch_size\": BATCH_SIZE,             # Размер батча для тренировки\n",
    "    \"per_device_eval_batch_size\": BATCH_SIZE,              # Размер батча для валидации\n",
    "    \"num_train_epochs\": 1,                        # Количество эпох обучения\n",
    "    \"weight_decay\": 0.01,                         # Коэффициент регуляризации             # Загружать лучшую модель в конце\n",
    "    \"logging_dir\": DATA_PATH / \"logs\",            # Папка для логов\n",
    "    \"max_length\": MAX_LENGTH,                            # Максимальная длина токенизированного текста\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = VaccineFakeClassifierTrainer(\n",
    "    model_name=\"roberta-base\",\n",
    "    num_labels=NUM_CLASSES,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    cache_dir=DATA_CACHE,\n",
    "    output_dir=DATA_PATH_SAVE_MODELS,\n",
    "    **training_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(\"Результаты на тестовом наборе:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logs = trainer.trainer.state.log_history\n",
    "\n",
    "# train_loss = [log[\"loss\"] for log in logs if \"loss\" in log]\n",
    "# val_loss = [log[\"eval_loss\"] for log in logs if \"eval_loss\" in log]\n",
    "# accuracy = [log[\"eval_accuracy\"] for log in logs if \"eval_accuracy\" in log]\n",
    "\n",
    "# logs_to_visualize = {\n",
    "#     \"train_loss\": train_loss,\n",
    "#     \"val_loss\": val_loss,\n",
    "#     \"accuracy\": accuracy,\n",
    "# }\n",
    "\n",
    "# trainer.visualize_metrics(logs_to_visualize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "\n",
    "class VaccineFakeClassifier:\n",
    "    def __init__(self, model_path: str, idx2label: Dict[int, str]):\n",
    "        \"\"\"\n",
    "        Инициализация модели и токенайзера для тестирования.\n",
    "        \n",
    "        :param model_path: Путь к дообученной модели.\n",
    "        :param idx2label: Словарь для сопоставления индексов классов с текстовыми метками.\n",
    "        \"\"\"\n",
    "        self.tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "        self.model: RobertaForSequenceClassification = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "        self.idx2label = idx2label\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def predict_class(self, text: str) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Определение класса текста и его вероятности.\n",
    "        \n",
    "        :param text: Входной текст.\n",
    "        :return: Текстовая метка класса и вероятность.\n",
    "        \"\"\"\n",
    "        # Токенизация текста\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        # Перенос тензоров на устройство модели\n",
    "        inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
    "        \n",
    "        # Предсказание\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predicted_idx = logits.argmax(axis=-1).item()\n",
    "            score = logits.softmax(dim=-1).max().item()  # Вероятность предсказанного класса\n",
    "        \n",
    "        return self.idx2label.get(predicted_idx, \"Unknown\"), score\n",
    "\n",
    "\n",
    "    def list_batch_predict(self, texts: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Определение классов для списка текстов и вероятностей.\n",
    "\n",
    "        :param texts: pandas Series с текстами.\n",
    "        :return: DataFrame с предсказанными метками и вероятностями.\n",
    "        \"\"\"\n",
    "        # Токенизация текстов\n",
    "        inputs = self.tokenizer(list(texts), return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        \n",
    "        # Перенос тензоров на устройство модели\n",
    "        inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
    "        \n",
    "        # Предсказание\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predicted_indices = logits.argmax(axis=-1).tolist()\n",
    "            scores = logits.softmax(dim=-1).max(dim=-1).values.tolist()  # Вероятности\n",
    "\n",
    "        # Формирование DataFrame с результатами\n",
    "        return pd.DataFrame({\n",
    "            \"predicted_label\": [self.idx2label.get(idx, \"Unknown\") for idx in predicted_indices],\n",
    "            \"score\": scores\n",
    "        })\n",
    "\n",
    "\n",
    "    def batch_predict(self, texts: pd.Series, batch_size: int = 16) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Определение классов для списка текстов и вероятностей с учётом размера батча.\n",
    "        \n",
    "        :param texts: pandas Series с текстами.\n",
    "        :param batch_size: Размер батча для обработки.\n",
    "        :return: DataFrame с предсказанными метками и вероятностями.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        scores = []\n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size].tolist()\n",
    "            inputs = self.tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "            inputs = {key: val.to(self.device) for key, val in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                batch_predictions = logits.argmax(axis=-1).tolist()\n",
    "                batch_scores = logits.softmax(dim=-1).max(dim=-1).values.tolist()\n",
    "\n",
    "                predictions.extend(batch_predictions)\n",
    "                scores.extend(batch_scores)\n",
    "\n",
    "        return pd.DataFrame({\n",
    "            \"predicted_label\": [self.idx2label.get(idx, \"Unknown\") for idx in predictions],\n",
    "            \"score\": scores\n",
    "        })\n",
    "\n",
    "    def test_model(self, test_df: pd.DataFrame, batch_size: int = 16) -> Tuple[Dict[str, float], pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Тестирование модели на тестовом DataFrame с учётом батчей.\n",
    "        \n",
    "        :param test_df: DataFrame с колонками 'text' и 'label'.\n",
    "        :param batch_size: Размер батча для обработки.\n",
    "        :return: Метрики точности и F1-меры, DataFrame с результатами.\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "        # Проверяем наличие необходимых колонок\n",
    "        if \"text\" not in test_df.columns or \"label\" not in test_df.columns:\n",
    "            raise ValueError(\"DataFrame должен содержать колонки 'text' и 'label'.\")\n",
    "\n",
    "        # Предсказания\n",
    "        predictions_df = self.batch_predict(test_df[\"text\"], batch_size=batch_size)\n",
    "\n",
    "        # Добавляем истинные метки и правильность предсказаний\n",
    "        results_df = test_df.copy()\n",
    "        results_df[\"predicted_label\"] = predictions_df[\"predicted_label\"]\n",
    "        results_df[\"score\"] = predictions_df[\"score\"]\n",
    "        results_df[\"correct\"] = results_df[\"label\"] == results_df[\"predicted_label\"].map({v: k for k, v in self.idx2label.items()})\n",
    "\n",
    "        # Вычисление метрик\n",
    "        accuracy = accuracy_score(results_df[\"label\"], results_df[\"predicted_label\"].map({v: k for k, v in self.idx2label.items()}))\n",
    "        f1 = f1_score(results_df[\"label\"], results_df[\"predicted_label\"].map({v: k for k, v in self.idx2label.items()}), average=\"weighted\")\n",
    "\n",
    "        return {\"accuracy\": accuracy, \"f1\": f1, \"results_df\": results_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = DATA_PATH_SAVE_MODELS / \"vaccine_fake_model\"\n",
    "\n",
    "idx2label = {\n",
    "    0: \"Real\",\n",
    "    1: \"Fake\"\n",
    "}\n",
    "\n",
    "classifier = VaccineFakeClassifier(model_path=model_path, idx2label=idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"About one in three adults used household cleaners and disinfectants unsafely to prevent such as using bleach on food products and improperly using household cleaners and disinfectants on hands or skin. Read more in.\"\n",
    "predicted_class = classifier.predict_class(text)\n",
    "print(f\"Класс текста: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение классов для нескольких текстов\n",
    "texts = [\n",
    "    \"About one in three adults used household cleaners and disinfectants unsafely to prevent such as using bleach on food products and improperly using household cleaners and disinfectants on hands or skin. Read more in\",\n",
    "    \"Drinking boiled hikaw-hikaw after doing suob/tuob or steam inhalation therapy will cure anyone with COVID-19\",\n",
    "    \"2nd week of Jan 🇨🇳 had mapped the genome &amp; shared it with WHO &amp; with wider 🌍. We rapidly published a “how to” on building a PCR test for from our partner lab in 🇩🇪. In the 3rd week WHO identified &amp; began contracting for validated production of these tests-\"\n",
    "]\n",
    "predicted_classes = classifier.list_batch_predict(texts)\n",
    "print(f\"Классы текстов: {predicted_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = classifier.test_model(test_df)\n",
    "\n",
    "print(\"Метрики:\", test_results['accuracy'],  test_results['f1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results['results_df'].sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
