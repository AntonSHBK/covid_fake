{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение определения фейковых фактов о COVID и вакцинации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path('data/')\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_CACHE = Path('data/cache_dir/')\n",
    "DATA_CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_PATH_SAVE_MODELS = Path('data/models/')\n",
    "DATA_PATH_SAVE_MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"covid_vaccine_fake_model\"\n",
    "TEST_DF_NAME = \"facebook_data_to_model.xlsx\"\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_excel(DATA_PATH / TEST_DF_NAME)\n",
    "data_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2label = {\n",
    "    0: \"Real\",\n",
    "    1: \"Fake\",\n",
    "    2: \"Comments\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import PreTrainedTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataframe: pd.DataFrame,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        max_length: int,\n",
    "        text_column: str = \"text\",\n",
    "        link_text_column: str = \"link_text\",\n",
    "        tensor_dtype: Tuple[torch.dtype, torch.dtype] = (torch.long, torch.long),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Инициализация датасета с ленивой токенизацией.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame с колонками для токенизации.\n",
    "            tokenizer (PreTrainedTokenizer): Токенайзер для преобразования текста.\n",
    "            max_length (int): Максимальная длина токенов.\n",
    "            text_column (str): Название основной текстовой колонки.\n",
    "            link_text_column (str): Название колонки с дополнительным текстом.\n",
    "            tensor_dtype (tuple): Типы данных для токенов (input_ids, attention_mask).\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe.copy()  # Копируем DataFrame, чтобы избежать изменений в оригинале\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.text_column = text_column\n",
    "        self.link_text_column = link_text_column\n",
    "        self.tensor_dtype = tensor_dtype\n",
    "\n",
    "        # Проверка, есть ли указанные колонки в DataFrame\n",
    "        if text_column not in dataframe.columns:\n",
    "            raise ValueError(f\"Колонка '{text_column}' отсутствует в DataFrame\")\n",
    "        if link_text_column not in dataframe.columns:\n",
    "            raise ValueError(f\"Колонка '{link_text_column}' отсутствует в DataFrame\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Возвращает количество примеров в датасете.\n",
    "        \"\"\"\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def _tokenize_text(self, text: str) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Токенизирует текст, если он не пустой, иначе возвращает тензоры с нулями.\n",
    "\n",
    "        Args:\n",
    "            text (str): Текст для токенизации.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, torch.Tensor]: Тензоры input_ids и attention_mask.\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or text.strip() == \"\":\n",
    "            return {\n",
    "                \"input_ids\": torch.zeros(self.max_length, dtype=self.tensor_dtype[0]),\n",
    "                \"attention_mask\": torch.zeros(self.max_length, dtype=self.tensor_dtype[1]),\n",
    "            }\n",
    "        else:\n",
    "            tokens = self.tokenizer(\n",
    "                text,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            return {\n",
    "                \"input_ids\": tokens[\"input_ids\"][0].to(dtype=self.tensor_dtype[0]),\n",
    "                \"attention_mask\": tokens[\"attention_mask\"][0].to(dtype=self.tensor_dtype[1]),\n",
    "            }\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Возвращает токенизированные данные.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Индекс примера.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, torch.Tensor]: Словарь с токенами из двух колонок.\n",
    "        \"\"\"\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        text_tokens = self._tokenize_text(row[self.text_column])\n",
    "\n",
    "        link_text_tokens = self._tokenize_text(row[self.link_text_column])\n",
    "\n",
    "        return {\n",
    "            \"input_ids_text\": text_tokens[\"input_ids\"],\n",
    "            \"attention_mask_text\": text_tokens[\"attention_mask\"],\n",
    "            \"input_ids_link\": link_text_tokens[\"input_ids\"],\n",
    "            \"attention_mask_link\": link_text_tokens[\"attention_mask\"],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(DATA_PATH_SAVE_MODELS / MODEL_NAME)\n",
    "\n",
    "dataset = TokenizedDataset(data_df, tokenizer, MAX_LENGTH, text_column='text', link_text_column='link_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    DATA_PATH_SAVE_MODELS / MODEL_NAME)\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def test_model_on_dataset(\n",
    "    model: torch.nn.Module,\n",
    "    dataset: TokenizedDataset,\n",
    "    idx2label: dict,\n",
    "    batch_size: int = 16,\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Прогоняет датасет через модель и добавляет предсказания в DataFrame.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Обученная модель.\n",
    "        dataset (Dataset): Токенизированный датасет.\n",
    "        idx2label (dict): Словарь, отображающий индексы категорий в названия.\n",
    "        batch_size (int): Размер батча для DataLoader.\n",
    "        device (torch.device): Устройство для вычислений (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame с добавленными предсказаниями:\n",
    "            - 'predict_1': предсказанная метка для основной колонки,\n",
    "            - 'probability_1': вероятность предсказания для основной колонки,\n",
    "            - 'predict_2': предсказанная метка для дополнительной колонки,\n",
    "            - 'probability_2': вероятность предсказания для дополнительной колонки.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Создаём DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    test_df = dataset.dataframe.copy()\n",
    "\n",
    "    # Списки для хранения предсказаний\n",
    "    predictions_1, probabilities_1 = [], []\n",
    "    predictions_2, probabilities_2 = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Processing\"):\n",
    "            batch_size_current = batch[\"input_ids_text\"].shape[0]\n",
    "\n",
    "            # Обрабатываем основную колонку (text)\n",
    "            input_ids_text = batch[\"input_ids_text\"].to(device)\n",
    "            attention_mask_text = batch[\"attention_mask_text\"].to(device)\n",
    "\n",
    "            text_has_content = [torch.any(input_ids_text[i] != 0).item() for i in range(batch_size_current)]\n",
    "            if any(text_has_content):  # Если хотя бы в одном есть текст\n",
    "                logits = model(input_ids=input_ids_text, attention_mask=attention_mask_text).logits\n",
    "                probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "                preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "                for i in range(batch_size_current):\n",
    "                    if text_has_content[i]:\n",
    "                        predictions_1.append(preds[i])\n",
    "                        probabilities_1.append(probs[i].tolist())\n",
    "                    else:\n",
    "                        predictions_1.append(None)\n",
    "                        probabilities_1.append(None)\n",
    "            else:\n",
    "                predictions_1.extend([None] * batch_size_current)\n",
    "                probabilities_1.extend([None] * batch_size_current)\n",
    "\n",
    "            # Обрабатываем дополнительную колонку (link_text)\n",
    "            input_ids_link = batch[\"input_ids_link\"].to(device)\n",
    "            attention_mask_link = batch[\"attention_mask_link\"].to(device)\n",
    "\n",
    "            link_text_has_content = [torch.any(input_ids_link[i] != 0).item() for i in range(batch_size_current)]\n",
    "            if any(link_text_has_content):  # Если хотя бы в одном есть текст\n",
    "                logits = model(input_ids=input_ids_link, attention_mask=attention_mask_link).logits\n",
    "                probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "                preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "                for i in range(batch_size_current):\n",
    "                    if link_text_has_content[i]:\n",
    "                        predictions_2.append(preds[i])\n",
    "                        probabilities_2.append(probs[i].tolist())\n",
    "                    else:\n",
    "                        predictions_2.append(None)\n",
    "                        probabilities_2.append(None)\n",
    "            else:\n",
    "                predictions_2.extend([None] * batch_size_current)\n",
    "                probabilities_2.extend([None] * batch_size_current)\n",
    "\n",
    "    # Преобразуем предсказания в DataFrame\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "    test_df[\"predict_1\"] = [idx2label[p] if p is not None else None for p in predictions_1]\n",
    "    test_df[\"probability_1\"] = probabilities_1\n",
    "    test_df[\"predict_2\"] = [idx2label[p] if p is not None else None for p in predictions_2]\n",
    "    test_df[\"probability_2\"] = probabilities_2\n",
    "\n",
    "    return test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df = test_model_on_dataset(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    idx2label=idx2label,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df['predict_1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df.to_excel(DATA_PATH / 'facebook_data_to_complate.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
